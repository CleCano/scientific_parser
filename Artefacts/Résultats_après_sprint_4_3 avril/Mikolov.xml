<article>
	<preamble>Mikolov.pdf</preamble>
	<titre>Efﬁcient Estimation of Word Representations in Vector Space</titre>
	<auteurs>
		<auteur>
			<nom>Tomas Mikolov</nom>
			<mail>tmikolov@google.com</mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract>  We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. </abstract>
	<introduction> Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several goodreasons - simplicity, robustness and the observation that simple models trained on huge amounts ofdata outperform complex systems trained on less data. An example is the popular N-gram modelused for statistical language modeling - today, it is possible to train N-grams on virtually all availabledata (trillions of words [3]).However, the simple techniques are at their limits in many tasks. For example, the amount ofrelevant in-domain data for automatic speech recognition is limited - the performance is usuallydominated by the size of high quality transcribed speech data (often just millions of words). Inmachine translation, the existing corpora for many languages contain only a few billions of wordsor less. Thus, there are situations where simple scaling up of the basic techniques will not result inany signiﬁcant progress, and we have to focus on more advanced techniques.With progress of machine learning techniques in recent years, it has become possible to train morecomplex models on much larger data set, and they typically outperform the simple models. Probablythe most successful concept is to use distributed representations of words [10]. For example, neuralnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].1.1 Goals of the PaperThe main goal of this paper is to introduce techniques that can be used for learning high-quality wordvectors from huge data sets with billions of words, and with millions of words in the vocabulary. Asfar as we know, none of the previously proposed architectures has been successfully trained on more1arXiv:1301.3781v3  [cs.CL]  7 Sep 2013than a few hundred of millions of words, with a modest dimensionality of the word vectors between50 - 100.We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but thatwords can have multiple degrees of similarity [20]. This has been observed earlier in the contextof inﬂectional languages - for example, nouns can have multiple word endings, and if we search forsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similarendings [13, 14].Somewhat surprisingly, it was found that similarity of word representations goes beyond simplesyntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].In this paper, we try to maximize accuracy of these vector operations by developing new modelarchitectures that preserve the linear regularities among words. We design a new comprehensive testset for measuring both syntactic and semantic regularities1, and show that many such regularitiescan be learned with high accuracy. Moreover, we discuss how training time and accuracy dependson the dimensionality of the word vectors and on the amount of the training data.1.2 Previous WorkRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular modelarchitecture for estimating neural network language model (NNLM) was proposed in [1], where afeedforward neural network with a linear projection layer and a non-linear hidden layer was used tolearn jointly the word vector representation and a statistical language model. This work has beenfollowed by many others.Another interesting architecture of NNLM was presented in [13, 14], where the word vectors areﬁrst learned using neural network with a single hidden layer. The word vectors are then used to trainthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In thiswork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors arelearned using a simple model.It was later shown that the word vectors can be used to signiﬁcantly improve and simplify manyNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using differentmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting wordvectors were made available for future research and comparison2. However, as far as we know, thesearchitectures were signiﬁcantly more computationally expensive for training than the one proposedin [13], with the exception of certain version of log-bilinear model where diagonal weight matricesare used [23]. </introduction>
	<discussion>  </discussion>
	<conclusion>  </conclusion>
	<biblio> [1] Y . Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155, 2003. [2] Y . Bengio, Y . LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Machines, MIT Press, 2007. [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007. [4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008. [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:24932537, 2011. [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V . Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y . Ng., Large Scale Distributed Deep Networks, NIPS, 2012. [7] J.C. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011. [8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990. [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y . Ng. Improving Word Representations via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational Linguistics, 2012. [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel distributed processing: Explorations in the microstructure of cognition. V olume 1: Foundations, MIT Press, 1986. [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012. [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y . Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011. [13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007. [14] T. Mikolov, J. Kopeck ´y, L. Burget, O. Glembek and J. ˇCernock ´y. Neural network based language models for higly inﬂective languages, In: Proc. ICASSP 2009. [15] T. Mikolov, M. Karaﬁát, L. Burget, J. ˇCernock ´y, S. Khudanpur. Recurrent neural network based language model, In: Proceedings of Interspeech, 2010. [16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock ´y, S. Khudanpur. Extensions of recurrent neural network language model, In: Proceedings of ICASSP 2011. [17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock ´y. Empirical Evaluation and Combination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011. 4The code is available at https://code.google.com/p/word2vec/ 11[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock ´y. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011. [19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012. [20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Representations. NAACL HLT 2013. [21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. Accepted to NIPS 2013. [22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML, 2007. [23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009. [24] A. Mnih, Y .W. Teh. A fast and simple algorithm for training neural probabilistic language models. ICML, 2012. [25] F. Morin, Y . Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005. [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by backpropagating errors. Nature, 323:533.536, 1986. [27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007. [28] R. Socher, E.H. Huang, J. Pennington, A.Y . Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011. [29] J. Turian, L. Ratinov, Y . Bengio. Word Representations: A Simple and General Method for Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010. [30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. International Joint Conference on Artiﬁcial Intelligence, 2005. [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for Measuring Relational Similarity. NAACL HLT 2013. [32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011. 12 </biblio>
</article>
