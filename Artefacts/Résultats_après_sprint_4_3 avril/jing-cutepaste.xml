<article>
	<preamble>jing-cutepaste.pdf</preamble>
	<titre>Cut and Paste Based Text Summarization</titre>
	<auteurs>
		<auteur>
			<nom>Hongyan Jing</nom>
			<mail></mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Kathleen R. McKeown</nom>
			<mail></mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract>  We present a cut and paste based text summa-  rizer, which uses operations derived from an anal-  ysis of human written abstracts. The summarizer  edits extracted sentences, using reduction to remove  inessential phrases and combination to merge re-  suiting phrases together as coherent sentences. Our  work includes a statistically based sentence decom-  position program that identifies where the phrases of  a summary originate in the original document, pro-  ducing an aligned corpus of summaries and articles  which we used to develop the summarizer.  </abstract>
	<introduction> There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals. Certainly one fac- tor contributing to this gap is that automatic sys- tems can not always correctly identify the important topics of an article. Another factor, however, which has received little attention, is that automatic sum- marizers have poor text generation techniques. Most automatic summarizers rely on extracting key sen- tences or paragraphs from an article to produce a summary. Since the extracted sentences are discon- nected in the original article, when they are strung together, the resulting summary can be inconcise, incoherent, and sometimes even misleading. We present a cut and paste based text sum- marization technique, aimed at reducing the gap between automatically generated summaries and human-written abstracts. Rather than focusing on how to identify key sentences, as do other re- searchers, we study how to generate the text of a summary once key sentences have been extracted. The main idea of cut and paste summarization is to reuse the text in an article to generate the summary. However, instead of simply extracting sentences as current summarizers do, the cut and paste system will "smooth" the extracted sentences by editing them. Such edits mainly involve cutting phrases and pasting them together in novel ways. The key features of this work are: (1) The identification of cutting and past- ing operations. We identified six operations that can be used alone or together to transform extracted sentences into sentences in human-written abstracts. The operations were identified based on manual and automatic comparison of human-written abstracts and the original articles. Examples include sentence reduction, sentence combination, syntactic transfor- mation, and lexical paraphrasing. (2) Development of an automatic system to perform cut and paste operations. Two opera- tions - sentence reduction and sentence combination - are most effective in transforming extracted sen- tences into summary sentences that are as concise and coherent as in human-written abstracts. We implemented a sentence reduction module that re- moves extraneous phrases from extracted sentences, and a sentence combination module that merges the extracted sentences or the reduced forms resulting from sentence reduction. Our sentence reduction model determines what to cut based on multiple sources of information, including syntactic knowl- edge, context, and statistics learned from corpus analysis. It improves the conciseness of extracted sentences, making them concise and on target. Our sentence combination module implements combina- tion rules that were identified by observing examples written by human professionals. It improves the co- herence of extracted sentences. (3) Decomposing human-wrltten summary sentences. The cut and paste technique we propose here is a new computational model which we based on analysis of human-written abstracts. To do this analysis, we developed an automatic system that can match a phrase in a human-written abstract to the corresponding phrase in the article, identifying its most likely location. This decomposition program allows us to analyze the construction of sentences in a human-written abstract. Its results have been used to train and test the sentence reduction and sentence combination module. In Section 2, we discuss the cut and paste tech- nique in general, from both a professional and com- putational perspective. We also describe the six cut and paste operations. In Section 3, we describe the 178 system architecture. The major components of the system, including sentence reduction, sentence com- bination, decomposition, and sentence selection, are described in Section 4. The evaluation results are shown in Section 5. Related work is discussed in Section 6. Finally, we conclude and discuss future work. Document sentence: When it arrives some- time next year in new TV sets, the V-chip will give parents a new and potentially revolution- ary device to block out programs they don't want their children to see. Summary sentence: The V-chip will give par- ents a device to block out programs they don't want their children to see.  </introduction>
	<discussion> (3) Corpus evidence. The program uses a cor- pus of input articles and their corresponding reduced forms in human-written abstracts to learn which components of a sentence or a phrase can be re- moved and how likely they are to be removed by professionals. This corpus was created using the de- composition program. We compute three types of probabilities from this corpus: the probability that a phrase is removed; the probability that a phrase is reduced (i.e., the phrase is not removed as a whole, but some components in the phrase are removed); and the probability that a phrase is unchanged at all (i.e., neither removed nor reduced). These cor- pus probabilities help us capture human practice. (4) Final decision. The final reduction decision is based on the results from all the earlier steps. A phrase is removed only if it is not grammatically obligatory, not the focus of the local context (indi- cated by a low context importance score), and has a reasonable probability of being removed by humans. The phrases we remove from an extracted sentence include clauses, prepositional phrases, gerunds, and to-infinitives. The result of sentence reduction is a shortened version of an extracted sentence 2. This shortened text can be used directly as a summary, or it can be fed to the sentence combination module to be merged with other sentences. Figure 3 shows two examples produced by the re- duction program. The corresponding sentences in human-written abstracts are also provided for com- parison. 2It is actually also possible that the reduction program decides no phrase in a sentence should be removed, thus the result of reduction is the same as the input. sentence reduction program 4.3 Sentence combination To build the combination module, we first manu- ally analyzed a corpus of combination examples pro- duced by human professionals, automatically cre- ated by the decomposition program, and identified a list of combination operations. Table 1 shows the combination operations. To implement a combination operation, we need to do two things: decide when to use which com- bination operation, and implement the combining actions. To decide when to use which operation, we analyzed examples by humans and manually wrote a set of rules. Two simple rules are shown in Fig- ure 4. Sample outputs using these two simple rules are shown in Figure 5. We are currently exploring using machine learning techniques to learn the com- bination rules from our corpus. The implementation of the combining actions in- volves joining two parse trees, substituting a subtree with another, or adding additional nodes. We im- plemented these actions using a formalism based on Tree Adjoining Grammar (Joshi, 1987). 4.4 Extraction Module The extraction module is the front end of the sum- marization system and its role is to extract key sen- tences. Our method is primarily based on lexical re- lations. First, we link words in a sentence with other words in the article through repetitions, morpholog- ical relations, or one of the lexical relations encoded in WordNet, similar to step 2 in sentence reduction. An importance score is computed for each word in a sentence based on the number of lexical links it has with other words, the type of links, and the direc- tions of the links. After assigning a score to each word in a sentence, we then compute a score for a sentence by adding up the scores for each word. This score is then normal- 182 Categories Combination Operations Add descriptions or names for people or organizations Aggregations Substitute incoherent phrases Substitute phrases with more general or specific information add description (see Figure 5) add name extract common subjects or objects (see Figure 5) change one sentence to a clause add connectives (e.g., and or while) add punctuations (e.g., ";") substitute dangling anaphora substitute dangling noun phrases substitute adverbs (e.g., here) remove connectives substitute with more general information substitute with more specific information Mixed operations combination of any of above operations (see Figure 2) Table 1: Combination operations Rule 1: IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- tion of the person or the organization exists somewhere in the original article but is missing in the summary)) THEN" replace the phrase with the full name plus the full description Rule 2: IF: ((two sentences are close to each other in the original article) and (their subjects refer to the same entity) and (at least one of the sentences is the reduced form resulting from sentence reduc- tion)) THEN: merge the two sentences by removing the subject in the second sentence, and then com- bining it with the first sentence using connective "and". Figure 4: Sample sentence combination rules ized over the number of words a sentence contains. The sentences with high scores are considered im- portant. The extraction system selects sentences based on the importance computed as above, as well as other indicators, including sentence positions, cue phrases, and tf*idf scores.  </discussion>
	<conclusion> and future work This paper presents a novel architecture for text summarization using cut and paste techniques ob- served in human-written abstracts. In order to auto- matically analyze a large quantity of human-written abstracts, we developed a decomposition program. The automatic decomposition allows us to build large corpora for studying sentence reduction and sentence combination, which are two effective op- erations in cut and paste. We developed a sentence reduction module that makes reduction decisions us- ing multiple sources of knowledge. We also investi- gated possible sentence combination operations and implemented the combination module. A sentence extraction module was developed and used as the front end of the summarization system. We are preparing the task-based evaluation of the overall system. We also plan to evaluate the porta- bility of the system by testing it on another corpus. We will also extend the system to query-based sum- marization and investigate whether the system can be modified for multiple document summarization. Acknowledgment We thank IBM for licensing us the ESG parser and the MITRE corporation for licensing us the co- reference resolution system. This material is based upon work supported by the National Science Foun- dation under Grant No. IRI 96-19124 and IRI  </conclusion>
	<biblio>  </biblio>
</article>
