<article>
	<preamble>mikheev J02-3002.pdf</preamble>
	<titre>Periods, Capitalized Words, etc.</titre>
	<auteurs>
		<auteur>
			<nom>Andrei Mikheev</nom>
			<mail>mikheev@cogsci.ed.ac.uk</mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract> In this article we present an approach for tackling three important aspects of text normalization: sentence boundary disambiguation, disambiguation of capitalized words in positions wherecapitalization is expected, and identiﬁcation of abbreviations. As opposed to the two dominanttechniques of computing statistics or writing specialized grammars, our document-centered ap-proach works by considering suggestive local contexts and repetitions of individual words withina document. This approach proved to be robust to domain shifts and new lexica and produced per-formance on the level with the highest reported results. When incorporated into a part-of-speechtagger, it helped reduce the error rate signiﬁcantly on capitalized words and sentence boundaries.We also investigated the portability to other languages and obtained encouraging results. </abstract>
	<introduction> Disambiguation of sentence boundaries and normalization of capitalized words, aswell as identiﬁcation of abbreviations, however small in comparison to other tasksof text processing, are of primary importance in the developing of practical text-processing applications. These tasks are usually performed before actual “intelligent”text processing starts, and errors made at this stage are very likely to cause more errorsat later stages and are therefore very dangerous.Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but infact it plays an important role in many tasks. In mixed-case texts capitalized wordsusually denote proper names (names of organizations, locations, people, artifacts, etc.),but there are special positions in the text where capitalization is expected. Such manda-tory positions include the ﬁrst word in a sentence, words in titles with all signiﬁcantwords capitalized or table entries, a capitalized word after a colon or open quote, andthe ﬁrst word in a list entry, among others. Capitalized words in these and some otherpositions present a case of ambiguity: they can stand for proper names, as in Whitelater said ..., or they can be just capitalized common words, as in White elephants are.... The disambiguation of capitalized words in ambiguous positions leads to theidentiﬁcation of proper names (or their derivatives), and in this article we will usethese two terms and the term case normalization interchangeably.Church (1995, p. 294) studied, among other simple text normalization techniques,the effect of case normalization for different words and showed that “sometimes casevariants refer to the same thing ( hurricane and Hurricane ), sometimes they refer todifferent things ( continental and Continental ) and sometimes they don’t refer to muchof anything (e.g., anytime and Anytime ).” Obviously these differences arise becausesome capitalized words stand for proper names (such as Continental , the name of anairline) and some do not.∗Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place,Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk290Computational Linguistics Volume 28, Number 3Proper names are the main concern of the named-entity recognition subtask (Chinchor 1998) of information extraction. The main objective of this subtask is the identi-ﬁcation of proper names and also their classiﬁcation into semantic categories (person,organization, location, etc.).1There the disambiguation of the ﬁrst word in a sentence(and in other ambiguous positions) is one of the central problems: about 20% of namedentities occur in ambiguous positions. For instance, the word Black in the sentenceinitial position can stand for a person’s surname but can also refer to the color. Evenin multiword capitalized phrases, the ﬁrst word can belong to the rest of the phraseor can be just an external modiﬁer. In the sentence Daily, Mason and Partners lost theircourt case , it is clear that Daily, Mason and Partners is the name of a company. In thesentence Unfortunately, Mason and Partners lost their court case , the name of the companydoes not include the word Unfortunately , but the word Daily is just as common a wordasUnfortunately .Identiﬁcation of proper names is also important in machine translation, becauseusually proper names are transliterated (i.e., phonetically translated) rather than prop-erly (semantically) translated. In conﬁdential texts, such as medical records, propernames must be identiﬁed and removed before making such texts available to peopleunauthorized to have access to personally identiﬁable information. And in general,most tasks that involve text analysis will beneﬁt from the robust disambiguation ofcapitalized words into proper names and common words.Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting. Segmenting text into sentences is an important aspectin developing many applications: syntactic parsing, information extraction, machinetranslation, question answering, text alignment, document summarization, etc. Sen-tence splitting in most cases is a simple matter: a period, an exclamation mark, or aquestion mark usually signals a sentence boundary. In certain cases, however, a perioddenotes a decimal point or is a part of an abbreviation, and thus it does not necessarilysignal a sentence boundary. Furthermore, an abbreviation itself can be the last tokenin a sentence in which case its period acts at the same time as part of this abbreviationand as the end-of-sentence indicator (fullstop). A detailed introduction to the SBDproblem can be found in Palmer and Hearst (1997).The disambiguation of capitalized words and sentence boundaries presents achicken-and-egg problem. If we know that a capitalized word that follows a period isa common word, we can safely assign such period as sentence terminal. On the otherhand, if we know that a period is not sentence terminal, then we can conclude thatthe following capitalized word is a proper name.Another frequent source of ambiguity in end-of-sentence marking is introduced byabbreviations: if we know that the word that precedes a period is notan abbreviation,then almost certainly this period denotes a sentence boundary. If, however, this wordis an abbreviation, then it is not that easy to make a clear decision. This problem isexacerbated by the fact that abbreviations do not form a closed set; that is, one can-not list all possible abbreviations. Moreover, abbreviations can coincide with regularwords; for example, “in” can denote an abbreviation for “inches,” “no” can denote anabbreviation for “number,” and “bus” can denote an abbreviation for “business.”In this article we present a method that tackles sentence boundaries, capitalizedwords, and abbreviations in a uniform way through a document-centered approach.As opposed to the two dominant techniques of computing statistics about the wordsthat surround potential sentence boundaries or writing specialized grammars, our ap1 In this article we are concerned only with the identiﬁcation of proper names.291Mikheev Periods, Capitalized Words, etc.proach disambiguates capitalized words and abbreviations by considering suggestivelocal contexts and repetitions of individual words within a document. It then appliesthis information to identify sentence boundaries using a small set of rules. </introduction>
	<discussion> malization: sentence boundary disambiguation, disambiguation of capitalized wordswhen they are used in positions where capitalization is expected, and identiﬁcation ofabbreviations. The major distinctive features of our approach can be summarized asfollows:•We tackle the sentence boundary task only after we have fullydisambiguated the word on the left and the word on the right of apotential sentence boundary punctuation sign.•To disambiguate capitalized words and abbreviations, we useinformation distributed across the entire document rather than theirimmediate local context.•Our approach does not require manual rule construction or dataannotation for training. Instead, it relies on four word lists that can begenerated completely automatically from a raw (unlabeled) corpus.In this approach we do not try to resolve each ambiguous word occurrence individually. Instead, the system scans the entire document for the contexts in which the wordsin question are used unambiguously, and this gives it grounds, acting by analogy, forresolving ambiguous contexts.We deliberately shaped our approach so that it largely does not rely on precompiled statistics, because the most interesting events are inherently infrequent and henceare difﬁcult to collect reliable statistics for. At the same time precompiled statisticswould be smoothed across multiple documents rather than targeted to a speciﬁc docu-ment. By collecting suggestive instances of usage for target words from each particulardocument on the ﬂy, rather than relying on preacquired resources smoothed across theentire document collection, our approach is robust to domain shifts and new lexicaand closely targeted to each document.314Computational Linguistics Volume 28, Number 3A signiﬁcant advantage of this approach is that it can be targeted to new domainscompletely automatically, without human intervention. The four word lists that oursystem uses for its operation can be generated automatically from a raw corpus andrequire no human annotation. Although some SBD systems can be trained on relativelysmall sets of labeled examples, their performance in such cases is somewhat lower thantheir optimal performance. For instance, Palmer and Hearst (1997) report that the SATZsystem (decision tree variant) was trained on a set of about 800 labeled periods, whichcorresponds to a corpus of about 16,000 words. This is a relatively small training setthat can be manually marked in a few hours’ time. But the error rate (1.5%) of thedecision tree classiﬁer trained on this small sample was about 50% higher than thatwhen trained on 6,000 labeled examples (1.0%).The performance of our system does not depend on the availability of labeledtraining examples. For its “training,” it uses a raw (unannotated in any way) corpusof texts. Although it needs such a corpus to be relatively large (a few hundred thousandwords), this is normally not a problem, since when the system is targeted to a newdomain, such a corpus is usually already available at no extra cost. Therefore there is notrade-off between the amount of human labor and the performance of the system. Thisnot only makes retargeting of such system easier but also enables it to be operationalin a completely autonomous way: it needs only to be pointed to texts from a newdomain, and then it can retarget itself automatically.Although the DCA requires two passes through a document, the simplicity of theunderlying algorithms makes it reasonably fast. It processes about 3,000 words persecond using a Pentium II 400 MHz processor. This includes identiﬁcation of abbre-viations, disambiguation of capitalized words, and then disambiguation of sentenceboundaries. This is comparable to the speed of other preprocessing systems.3The operational speed is about 10% higher than the training speed because, apart from applyingthe system to the training corpus, training also involves collecting, thresholding, andsorting of the word lists—all done automatically but at extra time cost. Training onthe 300,000-word NYT text collection took about two minutes.Despite its simplicity, the performance of our approach was on the level withthe previously highest reported results on the same test collections. The error rateon sentence boundaries in the Brown corpus was not signiﬁcantly worse than thelowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpusour system performed slightly better than the combination of the Alembic and SATZsystems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Althoughthese error rates seem to be very small, they are quite signiﬁcant. Unlike general POStagging, in which it is unfair to expect an error rate of less than 2% because even humanannotators have a disagreement rate of about 3%, sentence boundaries are much lessambiguous (with a disagreement of about 1 in 5,000). This shows that an error rateof 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,one error in 200 periods means that there is one error in every two documents in theBrown corpus and one error in every four documents in the WSJ corpus.With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably “well-behaved” texts that consistently use capitalization (mixed case) and do not containmuch noisy data. Thus, for instance, we do not expect our system to perform wellon single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on </discussion>
	<conclusion>  </conclusion>
	<biblio> Aberdeen, John S., John D. Burger, David S. Day, Lynette Hirschman, PatriciaRobinson, and Marc Vilain. 1995. “Mitre:Description of the alembic system usedfor MUC-6.” In Proceedings of the Sixth Message Understanding Conference (MUC-6) , Columbia, Maryland, November. MorganKaufmann. Baldwin, Breck, Christine Doran, Jeffrey Reynar, Michael Niv, Bangalore Srinivas,and Mark Wasson. 1997. “EAGLE: Anextensible architecture for generallinguistic engineering.” In Proceedings of Computer-Assisted Information Searching onInternet (RIAO ’97) , Montreal, June. Baum, Leonard E. and Ted Petrie. 1966. Statistical inference for probabilisticfunctions of ﬁnite Markov chains. Annals of Mathematical Statistics 37:1559–1563. Bikel, Daniel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997.“Nymble: A high performance learningname-ﬁnder.” In Proceedings of the Fifth Conference on Applied Natural LanguageProcessing (ANLP’97) , pages 194–200. Washington, D.C., Morgan Kaufmann. Brill, Eric. 1995a. Transformation-based error-driven learning and naturallanguage parsing: A case study inpart-of-speech tagging. Computational Linguistics 21(4):543–565. Brill, Eric. 1995b. “Unsupervised learning of disambiguation rules for part of speechtagging.” In David Yarovsky and KennethChurch, editors, Proceedings of the Third Workshop on Very Large Corpora , pages 1–13, Somerset, New Jersey. Associationfor Computational Linguistics. Burnage, Gavin. 1990. CELEX: A Guide for Users . Centre for Lexical Information, Nijmegen, Netherlands.317Mikheev Periods, Capitalized Words, etc. Chinchor, Nancy. 1998. “Overview of MUC-7.” In Seventh Message Understanding Conference (MUC-7): Proceedings of aConference Held in Fairfax , April. Morgan Kaufmann. Church, Kenneth. 1988. “A stochastic parts program and noun-phrase parser forunrestricted text.” In Proceedings of the Second ACL Conference on Applied NaturalLanguage Processing (ANLP’88) , pages 136–143, Austin, Texas. Church, Kenneth. 1995. “One term or two?” InSIGIR’95, Proceedings of the 18th Annual International ACM SIGIR Conference onResearch and Development in InformationRetrieval , pages 310–318, Seattle, Washington, July. ACM Press. Clarkson, Philip and Anthony J. Robinson. 1997. “Language model adaptation usingmixtures and an exponentially decayingcache.” In Proceedings IEEE International Conference on Speech and Signal Processing , Munich, Germany. Cucerzan, Silviu and David Yarowsky. 1999. “Language independent named entityrecognition combining morphological andcontextual evidence.” In Proceedings of Joint SIGDAT Conference on EMNLP andVLC. Francis, W. Nelson and Henry Kucera. 1982. Frequency Analysis of English Usage: Lexiconand Grammar . Houghton Mifﬂin, New York. Gale, William, Kenneth Church, and David Yarowsky. 1992. “One sense perdiscourse.” In Proceedings of the Fourth DARP A Speech and Natural LanguageWorkshop , pages 233–237. Grefenstette, Gregory and Pasi Tapanainen. 1994. “What is a word, what is asentence? Problems of tokenization.” InThe Proceedings of Third Conference onComputational Lexicography and TextResearch (COMPLEX’94) , Budapest, Hungary. Krupka, George R. and Kevin Hausman. 1998. Isoquest Inc.: Description of thenetowl extractor system as used forMUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7) , Fairfax, VA. Morgan Kaufmann. Kuhn, Roland and Renato de Mori. 1998. A cache-based natural language model forspeech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence12:570–583. Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden Markov model.Computer Speech and Language . Mani, Inderjeet and T. Richard MacMillan. 1995. “Identifying unknown propernames in newswire text.” In B. Boguraev and J. Pustejovsky, editors, Corpus Processing for Lexical Acquisition . MIT Press, Cambridge, Massachusetts, pages 41–59. Marcus, Mitchell, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building alarge annotated corpus of English: ThePenn treebank. Computational Linguistics 19(2):313–329. Mikheev, Andrei. 1997. Automatic rule induction for unknown word guessing.Computational Linguistics 23(3):405–423. Mikheev, Andrei. 1999. A knowledge-free method for capitalized worddisambiguation. In Proceedings of the 37th Conference of the Association forComputational Linguistics (ACL’99) , pages 159–168, University of Maryland, CollegePark. Mikheev, Andrei. 2000. “Tagging sentence boundaries.” In Proceedings of the First Meeting of the North American Chapter of theComputational Linguistics (NAACL’2000) , pages 264–271, Seattle, Washington.Morgan Kaufmann. Mikheev, Andrei, Clair Grover, and Colin Matheson. 1998. TTT: Text Tokenisation Tool . Language Technology Group, Universityof Edinburgh. Available athttp://www.ltg.ed.ac.uk/software/ttt/index.html. Mikheev, Andrei, Clair Grover, and Marc Moens. 1998. Description of the ltgsystem used for MUC-7. In Seventh Message Understanding Conference(MUC–7): Proceedings of a Conference Held inFairfax , Virginia. Morgan Kaufmann. Mikheev, Andrei and Liubov Liubushkina. 1995. Russian morphology: Anengineering approach. Natural Language Engineering 1(3):235–260. Palmer, David D. and Marti A. Hearst. 1994. “Adaptive sentence boundarydisambiguation.” In Proceedings of the Fourth ACL Conference on Applied NaturalLanguage Processing (ANLP’94) , pages 78–83, Stuttgart, Germany, October.Morgan Kaufmann. Palmer, David D. and Marti A. Hearst. 1997. Adaptive multilingual sentence boundarydisambiguation. Computational Linguistics 23(2):241–269. Park, Youngja and Roy J. Byrd. 2001. “Hybrid text mining for ﬁndingabbreviations and their deﬁnitions.” InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMLP’01) , pages 16–19, Washington, D.C. Morgan Kaufmann. Ratnaparkhi, Adwait. 1996. “A maximum entropy model for part-of-speech318Computational Linguistics Volume 28, Number 3 tagging.” In Proceedings of Conference on Empirical Methods in Natural LanguageProcessing , pages 133–142, University of Pennsylvania, Philadelphia. Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997. “A maximum entropy approach toidentifying sentence boundaries.” InProceedings of the Fifth ACL Conference onApplied Natural Language Processing(ANLP’97) , pages 16–19. Morgan Kaufmann. Riley, Michael D. 1989. “Some applications of tree-based modeling to speech andlanguage indexing.” In Proceedings of the DARP A Speech and Natural LanguageWorkshop , pages 339–352. Morgan Kaufmann. Yarowsky, David. 1993. “One sense per collocation.” In Proceedings of ARP A Human Language Technology Workshop ’93 , pages 266–271, Princeton, New Jersey. Yarowsky, David. 1995. “Unsupervised word sense disambiguation rivalingsupervised methods.” In Meeting of the Association for Computational Linguistics(ACL’95) , pages 189–196. </biblio>
</article>
