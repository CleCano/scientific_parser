<article>
	<preamble>Torres.pdf</preamble>
	<titre>Summary Evaluation with and without References</titre>
	<auteurs>
		<auteur>
			<nom>Juan-Manuel</nom>
			<mail>juan-manuel.torres@univ-avignon.fr</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Torres-Moreno</nom>
			<mail>eric.sanjuan@univ-avignon.fr</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Horacio Saggion</nom>
			<mail>horacio.saggion@upf.edu</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Iria da Cunha</nom>
			<mail>iria.dacunha@upf.edu</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Eric SanJuan</nom>
			<mail>velazquez@yahoo.com</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Patricia Velázquez-Morales</nom>
			<mail></mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract> We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE , RESPONSIVENESS ,PYRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms —Text summarization evaluation, content-based evaluation measures, divergences. </abstract>
	<introduction>  </introduction>
	<discussion>  </discussion>
	<conclusion>  </conclusion>
	<biblio> Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales Abstract —We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE , RESPONSIVENESS ,PYRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms —Text summarization evaluation, content-based evaluation measures, divergences. I. I NTRODUCTION TEXT summarization evaluation has always been a complex and controversial issue in computational linguistics. In the last decade, signiﬁcant advances have been made in this ﬁeld as well as various evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one, entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007. Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information access technologies including text summarization. Evaluation in text summarization can be extrinsic or intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an speciﬁc task carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm. In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July 25, 2010. Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon, France andÉcole Polytechnique de Montréal, Canada (juan-manuel.torres@univ-avignon.fr). Eric SanJuan is with LIA/Université d’Avignon, France (eric.sanjuan@univ-avignon.fr). Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Université d’Avignon, France and Instituto de Ingenier ´ıa/UNAM, Mexico (iria.dacunha@upf.edu). Patricia Velázquez-Morales is with VM Labs, France (patricia velazquez@yahoo.com).automatically generated summary ( peer) has to be compared with one or more reference summaries ( models ). DUC used an interface called SEE to allow human judges to compare apeer with a model . Thus, judges give a C OVERAGE score to each peer produced by a system and the ﬁnal system COVERAGE score is the average of the C OVERAGE ’s scores asigned. These system’s C OVERAGE scores can then be used to rank summarization systems. In the case of query-focused summarization (e.g. when the summary should answer a question or series of questions) a R ESPONSIVENESS score is also assigned to each summary, which indicates how responsive the summary is to the question(s). Because manual comparison of peer summaries with model summaries is an arduous and costly process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries [5]. Various vocabulary overlap measures such as n-grams overlap or longest common subsequence between peer and model have also been proposed [6], [7]. The B LEU machine translation evaluation measure [8] has also been tested in summarization [9]. The DUC conferences adopted the R OUGE package for content-based evaluation [10]. R OUGE implements a series of recall measures based on n-gram co-occurrence between a peer summary and a set of model summaries. These measures are used to produce systems’ rank. It has been shown that system rankings, produced by some R OUGE measures (e.g., R OUGE -2, which uses 2-grams), have a correlation with rankings produced using C OVERAGE . In recent years the P YRAMIDS evaluation method [11] has been introduced. It is based on the distribution of “content” of a set of model summaries. Summary Content Units (SCUs) are ﬁrst identiﬁed in the model summaries, then each SCU receives a weight which is the number of models containing or expressing the same unit. Peer SCUs are identiﬁed in the peer, matched against model SCUs, and weighted accordingly. The P YRAMIDS score given to a peer is the ratio of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with the same number of SCUs as the peer. The P YRAMIDS scores can be also used for ranking summarization systems. [11] showed that P YRAMIDS scores produced reliable system rankings when multiple (4 or more) models were used and that P YRAMIDS rankings correlate with rankings produced by R OUGE -2 and R OUGE -SU2 (i.e. R OUGE with skip 2-grams). However, this method requires the creation 13 Polibits (42) 2010of models and the identiﬁcation, matching, and weighting of SCUs in both: models and peers. [12] evaluated the effectiveness of the Jensen-Shannon (JS) [13] theoretic measure in predicting systems ranks in two summarization tasks: query-focused and update summarization. They have shown that ranks produced by P YRAMIDS and those produced by JS measure correlate. However, they did not investigate the effect of the measure in summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English. In this paper we present a series of experiments aimed at a better understanding of the value of the JS divergence for ranking summarization systems. We have carried out experimentation with the proposed measure and we have veriﬁed that in certain tasks (such as those studied by [12]) there is a strong correlation among P YRAMIDS , RESPONSIVENESS and theJS divergence, but as we will show in this paper, there are datasets in which the correlation is not so strong. We also present experiments in Spanish and French showing positive correlation between the JS and R OUGE which is the de facto evaluation measure used in evaluation of non-English summarization. To the best of our knowledge this is the more extensive set of experiments interpreting the value of evaluation without human models. The rest of the paper is organized in the following way: First in Section II we introduce related work in the area of content-based evaluation identifying the departing point for our inquiry; then in Section III we explain the methodology adopted in our work and the tools and resources used for experimentation. In Section IV we present the experiments carried out together with the results. Section V discusses the results and Section VI concludes the paper and identiﬁes future work. II. R ELATED WORK One of the ﬁrst works to use content-based measures in text summarization evaluation is due to [5], who presented an evaluation framework to compare rankings of summarization systems produced by recall and cosine-based measures. They showed that there was weak correlation among rankings produced by recall, but that content-based measures produce rankings which were strongly correlated. This put forward the idea of using directly the full document for comparison purposes in text summarization evaluation. [6] presented a set of evaluation measures based on the notion of vocabulary overlap including n-gram overlap, cosine similarity, and longest common subsequence, and they applied them to multi-document summarization in English and Chinese. However, they did not evaluate the performance of the measures in different summarization tasks. [7] also compared various evaluation measures based on vocabulary overlap. Although these measures were able to separate random fromnon-random systems, no clear conclusion was reached on the value of each of the studied measures. Nowadays, a widespread summarization evaluation framework is R OUGE [14], which offers a set of statistics that compare peer summaries with models. It counts co-occurrences of n-grams in peer and models to derive a score. There are several statistics depending on the used n-grams and the text processing applied to the input texts (e.g., lemmatization, stop-word removal). [15] proposed a method of evaluation based on the use of “distances” or divergences between two probability distributions (the distribution of units in the automatic summary and the distribution of units in the model summary). They studied two different Information Theoretic measures of divergence: the Kullback-Leibler ( KL) [16] and Jensen-Shannon (JS) [13] divergences. KL computes the divergence between probability distributions PandQin the following way: DKL(PjjQ) =1 2X wPwlog2Pw Qw(1) WhileJS divergence is deﬁned as follows: DJS(PjjQ) =1 2X wPwlog22Pw Pw+Qw+Qwlog22Qw Pw+Qw (2) These measures can be applied to the distribution of units in system summaries Pand reference summaries Q. The value obtained may be used as a score for the system summary. The method has been tested by [15] over the DUC 2002 corpus for single and multi-document summarization tasks showing good correlation among divergence measures and both coverage and ROUGE rankings. [12] went even further and, as in [5], they proposed to compare directly the distribution of words in full documents with the distribution of words in automatic summaries to derive a content-based evaluation measure. They found a high correlation between rankings produced using models and rankings produced without models. This last work is the departing point for our inquiry into the value of measures that do not rely on human models. III. M ETHODOLOGY The followed methodology in this paper mirrors the one adopted in past work (e.g. [5], [7], [12]). Given a particular summarization task T,pdata points to be summarized with input material fIigp
