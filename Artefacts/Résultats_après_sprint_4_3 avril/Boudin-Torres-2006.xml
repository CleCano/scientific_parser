<article>
	<preamble>Boudin-Torres-2006.pdf</preamble>
	<titre>A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization</titre>
	<auteurs>
		<auteur>
			<nom>Florian Boudin</nom>
			<mail>florian.boudin@univ-avignon.fr</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Marc El-Bèze</nom>
			<mail>marc.elbeze@univ-avignon.fr</mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract>  We present S MMR , a scalable sentence scoring method for query-oriented update summarization. Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance. We show that S MMR achieves promising results on the DUC 2007 update corpus. </abstract>
	<introduction> Extensive experiments on query-oriented multidocument summarization have been carried outover the past few years. Most of the strategiesto produce summaries are based on an extraction method, which identiﬁes salient textual segments, most often sentences, in documents. Sentences containing the most salient concepts are selected, ordered and assembled according to theirrelevance to produce summaries (also called extracts) (Mani and Maybury, 1999).Recently emerged from the Document Understanding Conference (DUC) 20071, update summarization attempts to enhance summarizationwhen more information about knowledge acquiredby the user is available. It asks the following question: has the user already read documents on thetopic? In the case of a positive answer, producingan extract focusing on only new facts is of interest. In this way, an important issue is introduced:c/circlecopyrt2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Document Understanding Conferences are conductedsince 2000 by the National Institute of Standards and Technology (NIST), http://www-nlpir.nist.govredundancy with previously read documents (history) has to be removed from the extract.A natural way to go about update summarizationwould be extracting temporal tags (dates, elapsedtimes, temporal expressions...) (Mani and Wilson,2000) or to automatically construct the timelinefrom documents (Swan and Allan, 2000). Thesetemporal marks could be used to focus extracts onthe most recently written facts. However, most recently written facts are not necessarily new facts.Machine Reading (MR) was used by (Hickl etal., 2007) to construct knowledge representationsfrom clusters of documents. Sentences containing “new” information (i.e. that could not be inferred by any previously considered document)are selected to generate summary. However, thishighly efﬁcient approach (best system in DUC2007 update) requires large linguistic resources.(Witte et al., 2007) propose a rule-based systembased on fuzzy coreference cluster graphs. Again,this approach requires to manually write the sentence ranking scheme. Several strategies remaining on post-processing redundancy removal techniques have been suggested. Extracts constructedfrom history were used by (Boudin and TorresMoreno, 2007) to minimize history’s redundancy.(Lin et al., 2007) have proposed a modiﬁed Maximal Marginal Relevance (MMR) (Carbonell andGoldstein, 1998) re-ranker during sentence selection, constructing the summary by incrementallyre-ranking sentences.In this paper, we propose a scalable sentencescoring method for update summarization derivedfrom MMR. Motivated by the need for relevantnovelty, candidate sentences are selected according to a combined criterion of query relevance anddissimilarity with previously read sentences. Therest of the paper is organized as follows. Section 223introduces our proposed sentence scoring methodand Section 3 presents experiments and evaluatesour approach. </introduction>
	<discussion> In this paper we have described S MMR , a scalable sentence scoring method based on MMR thatachieves very promising results. An important aspect of our sentence scoring method is that it doesnot requires re-ranking nor linguistic knowledge,which makes it a simple and fast approach to theissue of update summarization. It was pointed outat the DUC 2007 workshop that Question Answering and query-oriented summarization have beenconverging on a common task. The value addedby summarization lies in the linguistic quality. Approaches mixing IR techniques are well suited forquery-oriented summarization but they require intensive work for making the summary ﬂuent andcoherent. Among the others, this is a point that wethink is worthy of further investigation.AcknowledgmentsThis work was supported by the Agence Nationalede la Recherche , France, project RPM2. </discussion>
	<conclusion>  </conclusion>
	<biblio> Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented Multi-Document Update Summarization. InRecent Advances in Natural Language Processing (RANLP) , pages 81–87. Carbonell, J. and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In 21st annual international ACM SIGIR conference on Research and development in information retrieval , pages 335–336. ACM Press New York, NY , USA.Hachey, B., G. Murray, and D. Reitter. 2005. The Embra System at DUC 2005: Query-oriented Multidocument Summarization with a Very Large Latent Semantic Space. In Document Understanding Conference (DUC) . Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC’s GISTexter at DUC 2007: Machine Reading for Update Summarization. In Document Understanding Conference (DUC) . Lin, Z., T.S. Chua, M.Y . Kan, W.S. Lee, L. Qiu, and S. Ye. 2007. NUS at DUC 2007: Using Evolutionary Models of Text. In Document Understanding Conference (DUC) . Lin, C.Y . 2004. Rouge: A Package for Automatic Evaluation of Summaries. In Workshop on Text Summarization Branches Out , pages 25–26. Mani, I. and M.T. Maybury. 1999. Advances in Automatic Text Summarization . MIT Press. Mani, I. and G. Wilson. 2000. Robust temporal processing of news. In 38th Annual Meeting on Association for Computational Linguistics , pages 69–76. Association for Computational Linguistics Morristown, NJ, USA. Murray, G., S. Renals, and J. Carletta. 2005. Extractive Summarization of Meeting Recordings. In Ninth European Conference on Speech Communication and Technology . ISCA. Salton, G., A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM , 18(11):613–620. Swan, R. and J. Allan. 2000. Automatic generation of overview timelines. In 23rd annual international ACM SIGIR conference on Research and development in information retrieval , pages 49–56. Winkler, W. E. 1999. The state of record linkage and current research problems. In Survey Methods Section, pages 73–79. Witte, R., R. Krestel, and S. Bergler. 2007. Generating Update Summaries for DUC 2007. In Document Understanding Conference (DUC) . Ye, S., L. Qiu, T.S. Chua, and M.Y . Kan. 2005. NUS at DUC 2005: Understanding documents via concept links. In Document Understanding Conference (DUC) .26 </biblio>
</article>
