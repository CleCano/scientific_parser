<article>
	<preamble>Torres-moreno1998.pdf</preamble>
	<titre>Efficient Adaptive Learning for Classification Tasks with Binary Units</titre>
	<auteurs>
		<auteur>
			<nom>J. Manuel</nom>
			<mail></mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Torres Moreno</nom>
			<mail></mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract> This article presents a new incremental learning algorithm for classiﬁcation tasks, called NetLines, which is well adapted for both binaryand real-valued input patterns. It generates small, compact feedforwardneural networks with one hidden layer of binary units and binary outputunits. A convergence theorem ensures that solutions with a ﬁnite num-ber of hidden units exist for both binary and real-valued input patterns.An implementation for problems with more than two classes, validfor any binary classiﬁer, is proposed. The generalization error andthe size of the resulting networks are compared to the best publishedresults on well-known classiﬁcation benchmarks. Early stopping is shownto decrease overﬁtting, without improving the generalization perfor-mance. </abstract>
	<introduction> Feedforward neural networks have been successfully applied to the problem of learning pattern classiﬁcation from examples. The relationship of thenumber of weights to the learning capacity and the network’s generalizationability is well understood only for the simple perceptron, a single binaryunit whose output is a sigmoidal function of the weighted sum of its inputs.In this case, efﬁcient learning algorithms based on theoretical results allowthe determination of the optimal weights. However, simple perceptrons cangeneralize only those (very few) problems in which the input patterns arelinearly separable (LS). In many actual classiﬁcation tasks, multilayered per-ceptrons with hidden units are needed. However, neither the architecture(number of units, number of layers) nor the functions that hidden unitshave to learn are known a priori, and the theoretical understanding of thesenetworks is not enough to provide useful hints.Although pattern classiﬁcation is an intrinsically discrete task, it may becast as a problem of function approximation or regression by assigning realvalues to the targets. This is the approach used by backpropagation andNeural Computation 10, 1007–1030 (1998) c°1998 Massachusetts Institute of Technology1008 J. Manuel Torres Moreno and Mirta B. Gordonrelated algorithms, which minimize the squared training error of the output units. The approximating function must be highly nonlinear because ithas to ﬁt a constant value inside the domains of each class and present alarge variation at the boundaries between classes. For example, in a binaryclassiﬁcation task in which the two classes are coded as C1 and¡1, theapproximating function must be constant and positive in the input spaceregions or domains corresponding to class 1 and constant and negativefor those of class¡1. The network’s weights are trained to ﬁt this functioneverywhere—in particular, inside the class domains—instead of concentrat-ing on the relevant problem of the determination of the frontiers betweenclasses. Because the number of parameters needed for the ﬁt is not knowna priori, it is tempting to train a large number of weights that can span, atleast in principle, a large set of functions expected to contain the “true” one.This introduces a small bias (Geman, Bienenstock, & Doursat, 1992), butleaves us with the difﬁcult problem of minimizing a cost function in a high-dimensional space, with the risk that the algorithm gets stuck in spuriouslocal minima, whose number grows with the number of weights. In prac-tice, the best generalizer is determined through a trial-and-error process inwhich both the numbers of neurons and weights are varied.An alternative approach is provided by incremental, adaptive, or growthalgorithms, in which the hidden units are successively added to the network.One advantage is fast learning, not only because the problem is reduced totraining simple perceptrons but also because adaptive procedures do notneed the trial-and-error search for the most convenient architecture. Growthalgorithms allow the use of binary hidden neurons, well suited for buildinghardware-dedicated devices. Each binary unit determines a domain bound-ary in input space. Patterns lying on either side of the boundary are givendifferent hidden states. Thus, all the patterns inside a domain in input spaceare mapped to the same internal representation (IR). This binary encoding isdifferent for each domain. The output unit performs a logic (binary) functionof these IRs, a feature that may be useful for rule extraction. Because thereis not a unique way of associating IRs to the input patterns, different incre-mental learning algorithms propose different targets to be learned by theappended hidden neurons. This is not the only difference. Several heuristicsexist that generate fully connected feedforward networks with one or morelayers, and treelike architectures with different types of neurons (linear, ra-dial basis functions). Most of these algorithms are not optimal with respectto the number of weights or hidden units. Indeed, growth algorithms haveoften been criticized because they may generate networks that are too large,generally believed to be poor generalizers because of overﬁtting.This article presents a new incremental learning algorithm for binaryclassiﬁcation tasks that generates small feedforward networks. These net-works have a single hidden layer of binary neurons fully connected to theinputs and a single output neuron connected to the hidden units. We callitNetLines , for Neural Encoder Through Linear Separations. During theClassiﬁcation Tasks with Binary Units 1009learning process, the targets that each appended hidden unit has to learnhelp to decrease the number of classiﬁcation errors of the output neuron.The crucial test for any learning algorithm is the generalization ability ofthe resulting network. It turns out that the networks built with NetLines aregenerally smaller and generalize better than the best networks found so faron well-known benchmarks. Thus, large networks do not necessarily fol-low from growth heuristics. On the other hand, although smaller networksmay be generated with NetLines through early stopping, we found thatthey do not generalize better than the networks that were trained until thenumber of training errors vanished. Thus, overﬁtting does not necessarilyspoil the network’s performance. This surprising result is in good agreementwith recent work on the bias-variance dilemma (Friedman, 1996) showingthat, unlike in regression problems where bias and variance compete in thedetermination of the optimal generalizer, in the case of classiﬁcation theycombine in a highly nonlinear way.Although NetLines creates networks for two-class problems, multiclassproblems may be solved using any strategy that combines binary classiﬁers,like winner-takes-all. We propose a more involved approach, through theconstruction of a tree of networks, that may be coupled with any binaryclassiﬁer.NetLines is an efﬁcient approach for creating small, compact classiﬁersfor problems with binary or continuous inputs. It is best suited for problemsrequiring a discrete classiﬁcation decision. Although it may estimate poste-rior probabilities, as discussed in section 2.6, this requires more informationthan the bare network’s output. Another weakness of NetLines is that it isnot simple to retrain the network when new patterns are available or classpriors change over time.In section 2, we give the basic deﬁnitions and present a simple exampleof our strategy, followed by the formal presentation of the growth heuristicsand the perceptron learning algorithm used to train the individual units.In section 3 we compare NetLines to other growth strategies. The construc-tion of trees of networks for multiclass problems is presented in section 4.A comparison of the generalization error and the network’s size, with re-sults obtained with other learning procedures, is presented in section 5. Theconclusions are set out in section 6. </introduction>
	<discussion>  </discussion>
	<conclusion>  </conclusion>
	<biblio> Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Unpublished doctoral dissertation, Ecole Polytechnique Fédérale de Lausanne, Switzerland. Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical Review A, 44 , 6888. Bottou, L., & Vapnik, V . (1992). Local learning algorithms. Neural Computation, 4(6), 888–900. Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department of Statistics, University of California at Berkeley. Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classiﬁcation and regression trees . Monterey, CA: Wadsworth and Brooks/Cole. Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopﬁeld, J. (1987). Large automatic learning, rule extraction, and generalization. Complex Systems, 1 , 877–922. Depenau, J. (1995). Automated design of neural network architecture for classiﬁcation. Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity. Drucker, H., Schapire, R., & Simard, P . (1993). Improving performance in neural networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42– 49). San Mateo, CA: Morgan Kaufmann. Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architecture. In D. S. Touretzky (Ed.), Advances in neural information processing systems, 2(pp. 524–532). San Mateo: Morgan Kaufmann. Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree networks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing Systems, 6 (pp. 1035–1042). San Mateo, CA: Morgan Kaufmann. Frean, M. (1990). The Upstart algorithm: A method for constructing and training feedforward neural networks. Neural Computation, 2 (2), 198–209. Frean, M. (1992). A “thermal” perceptron learning rule. Neural Computation, 4 (6), 946–957. Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality (Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University. Fritzke, B. (1994). Supervised learning with growing cell structures. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural information processing systems, 6 (pp. 255–262). San Mateo, CA: Morgan Kaufmann. Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern Recognition, Oct. 28–31, Paris , vol. 4.Classiﬁcation Tasks with Binary Units 1029 Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.). 5èmes Journées Nationales du PRC-IA Teknea, Nancy. Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the bias/variance dilemma. Neural Computation, 4 (1), 1–58. Goodman, R. M., Smyth, P ., Higgins, C. M., & Miller, J. W. (1992). Rule-based neural networks for classiﬁcation and probability estimation. Neural Computation, 4 (6), 781–804. Gordon, M. B. (1996). A convergence theorem for incremental learning with realvalued inputs. In IEEE International Conference on Neural Networks , pp. 381– 386. Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule that ﬁnds the optimal weights. In M. Verleysen (Ed.), European Symposium on Artiﬁcial Neural Networks (pp. 105–110). Brussels: D Facto. Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature dependent algorithm. Europhysics Letters, 29 (3), 257–262. Gordon, M. B., Peretto, P ., & Berchier, D. (1993). Learning algorithms for perceptrons from statistical physics. Journal of Physics I (France), 3 , 377–387. Gorman, R. P ., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered network trained to classify sonar targets. Neural Networks, 1 , 75–89. Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In W. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses . Singapore: World Scientiﬁc. Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh: Carnegie Mellon University. Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A stepwise procedure for building and training a neural network. In J. Hérault & F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications (pp. 41–50). Berlin: Springer-Verlag. Marchand, M., Golea, M., & Ruján, P . (1990). A convergence theorem for sequential learning in two-layer perceptrons. Europhysics Letters, 11 , 487–492. Martinez, D., & Estève, D. (1992). The offset algorithm: Building and learning method for multilayer neural networks. Europhysics Letters, 18 , 95–100. Mézard, M., & Nadal, J.-P . (1989). Learning in feedforward layered networks: The Tiling algorithm. J. Phys. A: Math. and Gen., 22 , 2191–2203. Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time algorithm for generating neural networks for pattern classiﬁcation: Its stabilityproperties and some test results. Neural Computation, 5 (2), 317–330. Nadal, J.-P . (1989). Study of a growth algorithm for a feedforward neural network. Int. J. Neur. Syst., 1 , 55–59. Prechelt, L. (1994). PROBEN1—A set of benchmarks and benchmarking rules for neural network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe, Faculty of Informatics. Rafﬁn, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror, a temperature dependent learning algorithm. Neural Computation, 7 (6), 1206– 1224.1030 J. Manuel Torres Moreno and Mirta B. Gordon Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category learning. Biological Cybernetics, 45 , 35–41. Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm for the construction and training of a class of multilayer perceptron. Neural Networks, 6 (1), 535–545. Sirat, J. A., & Nadal, J.-P . (1990). Neural trees: A new tool for classiﬁcation. Network, 1 , 423–438. Solla, S. A. (1989). Learning and generalization in layered neural networks: The contiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks from Models to Applications . Paris: I.D.S.E.T. Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled with optimal perceptron learning for classiﬁcation. In M. Verleysen (Ed.),European Symposium on Artiﬁcial Neural Networks . Brussels: D Facto. Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar signals benchmark. Neural Proc. Letters, 7 (1), 1–4. Trhun, S. B., et al. (1991). The monk’s problems: A performance comparison of different learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie Mellon University. Vapnik, V . (1992). Principles of risk minimization for learning theory. In J. E. Moody, S. J. Hanson, & R. P . Lippmann (Eds.), Advances in neural information processing systems, 4 (pp. 831–838). San Mateo, CA: Morgan Kaufmann. Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neural networks. In M. Verleysen (Ed.), European Symposium on Artiﬁcial Neural Networks (pp. 359–364). Brussels: D Facto. Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. In Proceedings of the National Academy of Sciences, USA, 87 , 9193–9196. Received February 13, 1997; accepted September 4, 1997.This article has been cited by: 1.C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral analysis. IEEE Transactions on Neural Networks  10, 725-740. [ CrossRef ] 2.Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural Networks. International Journal of Neural Systems  08, 647-659. [ CrossRef ] </biblio>
</article>
