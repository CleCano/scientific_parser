LETTER Communicated by Scott Fahlman
Efﬁcient Adaptive Learning for Classiﬁcation Tasks with
Binary Units
J. Manuel Torres Moreno
Mirta B. Gordon
D´epartement de Recherche Fondamentale sur la Mati `ere Condens ´ee, CEA Grenoble,
38054 Grenoble Cedex 9, France
This article presents a new incremental learning algorithm for classi-
ﬁcation tasks, called NetLines, which is well adapted for both binaryand real-valued input patterns. It generates small, compact feedforwardneural networks with one hidden layer of binary units and binary outputunits. A convergence theorem ensures that solutions with a ﬁnite num-ber of hidden units exist for both binary and real-valued input patterns.An implementation for problems with more than two classes, validfor any binary classiﬁer, is proposed. The generalization error andthe size of the resulting networks are compared to the best publishedresults on well-known classiﬁcation benchmarks. Early stopping is shownto decrease overﬁtting, without improving the generalization perfor-mance.
1 Introduction
Feedforward neural networks have been successfully applied to the prob-
lem of learning pattern classiﬁcation from examples. The relationship of thenumber of weights to the learning capacity and the network’s generalizationability is well understood only for the simple perceptron, a single binaryunit whose output is a sigmoidal function of the weighted sum of its inputs.In this case, efﬁcient learning algorithms based on theoretical results allowthe determination of the optimal weights. However, simple perceptrons cangeneralize only those (very few) problems in which the input patterns arelinearly separable (LS). In many actual classiﬁcation tasks, multilayered per-ceptrons with hidden units are needed. However, neither the architecture(number of units, number of layers) nor the functions that hidden unitshave to learn are known a priori, and the theoretical understanding of thesenetworks is not enough to provide useful hints.
Although pattern classiﬁcation is an intrinsically discrete task, it may be
cast as a problem of function approximation or regression by assigning realvalues to the targets. This is the approach used by backpropagation and
Neural Computation 10, 1007–1030 (1998) c°1998 Massachusetts Institute of Technology1008 J. Manuel Torres Moreno and Mirta B. Gordon
related algorithms, which minimize the squared training error of the out-
put units. The approximating function must be highly nonlinear because ithas to ﬁt a constant value inside the domains of each class and present alarge variation at the boundaries between classes. For example, in a binaryclassiﬁcation task in which the two classes are coded as C1 and¡1, the
approximating function must be constant and positive in the input spaceregions or domains corresponding to class 1 and constant and negativefor those of class¡1. The network’s weights are trained to ﬁt this function
everywhere—in particular, inside the class domains—instead of concentrat-ing on the relevant problem of the determination of the frontiers betweenclasses. Because the number of parameters needed for the ﬁt is not knowna priori, it is tempting to train a large number of weights that can span, atleast in principle, a large set of functions expected to contain the “true” one.This introduces a small bias (Geman, Bienenstock, & Doursat, 1992), butleaves us with the difﬁcult problem of minimizing a cost function in a high-dimensional space, with the risk that the algorithm gets stuck in spuriouslocal minima, whose number grows with the number of weights. In prac-tice, the best generalizer is determined through a trial-and-error process inwhich both the numbers of neurons and weights are varied.
An alternative approach is provided by incremental, adaptive, or growth
algorithms, in which the hidden units are successively added to the network.One advantage is fast learning, not only because the problem is reduced totraining simple perceptrons but also because adaptive procedures do notneed the trial-and-error search for the most convenient architecture. Growthalgorithms allow the use of binary hidden neurons, well suited for buildinghardware-dedicated devices. Each binary unit determines a domain bound-ary in input space. Patterns lying on either side of the boundary are givendifferent hidden states. Thus, all the patterns inside a domain in input spaceare mapped to the same internal representation (IR). This binary encoding isdifferent for each domain. The output unit performs a logic (binary) functionof these IRs, a feature that may be useful for rule extraction. Because thereis not a unique way of associating IRs to the input patterns, different incre-mental learning algorithms propose different targets to be learned by theappended hidden neurons. This is not the only difference. Several heuristicsexist that generate fully connected feedforward networks with one or morelayers, and treelike architectures with different types of neurons (linear, ra-dial basis functions). Most of these algorithms are not optimal with respectto the number of weights or hidden units. Indeed, growth algorithms haveoften been criticized because they may generate networks that are too large,generally believed to be poor generalizers because of overﬁtting.
This article presents a new incremental learning algorithm for binary
classiﬁcation tasks that generates small feedforward networks. These net-works have a single hidden layer of binary neurons fully connected to theinputs and a single output neuron connected to the hidden units. We callitNetLines , for Neural Encoder Through Linear Separations. During theClassiﬁcation Tasks with Binary Units 1009
learning process, the targets that each appended hidden unit has to learn
help to decrease the number of classiﬁcation errors of the output neuron.The crucial test for any learning algorithm is the generalization ability ofthe resulting network. It turns out that the networks built with NetLines aregenerally smaller and generalize better than the best networks found so faron well-known benchmarks. Thus, large networks do not necessarily fol-low from growth heuristics. On the other hand, although smaller networksmay be generated with NetLines through early stopping, we found thatthey do not generalize better than the networks that were trained until thenumber of training errors vanished. Thus, overﬁtting does not necessarilyspoil the network’s performance. This surprising result is in good agreementwith recent work on the bias-variance dilemma (Friedman, 1996) showingthat, unlike in regression problems where bias and variance compete in thedetermination of the optimal generalizer, in the case of classiﬁcation theycombine in a highly nonlinear way.
Although NetLines creates networks for two-class problems, multiclass
problems may be solved using any strategy that combines binary classiﬁers,like winner-takes-all. We propose a more involved approach, through theconstruction of a tree of networks, that may be coupled with any binaryclassiﬁer.
NetLines is an efﬁcient approach for creating small, compact classiﬁers
for problems with binary or continuous inputs. It is best suited for problemsrequiring a discrete classiﬁcation decision. Although it may estimate poste-rior probabilities, as discussed in section 2.6, this requires more informationthan the bare network’s output. Another weakness of NetLines is that it isnot simple to retrain the network when new patterns are available or classpriors change over time.
In section 2, we give the basic deﬁnitions and present a simple example
of our strategy, followed by the formal presentation of the growth heuristicsand the perceptron learning algorithm used to train the individual units.In section 3 we compare NetLines to other growth strategies. The construc-tion of trees of networks for multiclass problems is presented in section 4.A comparison of the generalization error and the network’s size, with re-sults obtained with other learning procedures, is presented in section 5. Theconclusions are set out in section 6.
2 The Incremental Learning Strategy
2.1 Deﬁnitions. We are given a training set of Pinput-output examples
fE»„;¿„g, where„D1;2;:::; P. The inputsE»„D.1;»„
1;»„
2;:::;»„
N/may be
binary or real valued NC1 dimensional vectors. The ﬁrst component »„
0·1,
the same for all the patterns, allows us to treat the bias as a supplementaryweight. The outputs are binary, ¿
„D§ 1. These patterns are used to learn
the classiﬁcation task with the growth algorithm. Assume that, at a givenstage of the learning process, the network already has hbinary neurons1010 J. Manuel Torres Moreno and Mirta B. Gordon
in the hidden layer. These neurons are connected to the NC1 input units
through synaptic weights EwkD.wk0;wk1¢¢¢wkN/,1•k•h,wk0being the
bias.
Then, given an input pattern E», the states¾kof the hidden neurons (1 •
k•h) given by
¾kDsignˆNX
iD0wki»i!
·sign.Ewk¢E»/ (2.1)
deﬁne the pattern’s h-dimensional IR,E¾.h/D.1;¾1;:::;¾ h/. The network’s
output‡.h/is:
‡.h/DsignˆhX
kD0Wk¾k!
·signh
EW.h/¢E¾.h/i
(2.2)
whereEW.h/D.W0;W1;:::; Whare the output unit weights. Hereafter,
E¾„.h/D.1;¾„
1;:::;¾„
h/is the h-dimensional IR associated by the network
ofhhidden units to pattern E»„. During the training process, hincreases
through the addition of hidden neurons, and we denote the ﬁnal numberof hidden units as H.
2.2 Example. We ﬁrst describe the general strategy on a schematic ex-
ample (see Figure 1). Patterns in the gray region belong to class ¿DC1, the
others to¿D¡ 1. The algorithm proceeds as follows. A ﬁrst hidden unit
is trained to separate the input patterns at best and ﬁnds one solution, say
Ew
1, represented on Figure 1 by the line labeled 1, with the arrow pointing
into the positive half-space. Because training errors remain, a second hid-den neuron is introduced. It is trained to learn targets ¿
2DC1 for patterns
well classiﬁed by the ﬁrst neuron and ¿2D¡ 1 for the others (the opposite
convention could be adopted, both being strictly equivalent), and supposethat solutionEw
2is found. Then an output unit is connected to the two hid-
den neurons and is trained with the original targets. Clearly it will fail toseparate all the patterns correctly because the IR .¡1;1/and.C¡/are not
faithful, as patterns of both classes are mapped onto them. The output neu-ron is dropped, and a third hidden unit is appended and trained with targets¿
3DC1 for patterns that were correctly classiﬁed by the output neuron and
¿3D¡ 1 for the others. Solution Ew3is found, and it is easy to see that now
the IRs are faithful, that is, patterns belonging to different classes are givendifferent IRs. The algorithm converged with three hidden units that deﬁnethree domain boundaries determining six regions or domains in the inputspace. It is straightforward to verify that the IRs corresponding to each do-main on Figure 1 are linearly separable. Thus, the output unit will ﬁnd thecorrect solution to the training problem. If the faithful IRs were not linearlyseparable, the output unit would not ﬁnd a solution without training errors,and the algorithm would go on appending hidden units that should learnClassiﬁcation Tasks with Binary Units 1011
3
1
2- + -
+ - -- + + 
+ + +
+ - ++ + -
Figure 1: Patterns inside the gray region belong to one class, those in the white
region to the other. The lines (labeled 1, 2, and 3) represent the hyperplanes foundwith the NetLines strategy. The arrows point into the correspondent positivehalf-spaces. The IRs of each domain are indicated (the ﬁrst component, ¾
0D1,
is omitted for clarity).
targets¿D1 for well-learned patterns, and ¿D¡1 for the others. A proof
that a solution to this strategy with a ﬁnite number of hidden units exists isleft to the appendix.
2.3 The Algorithm NetLines. Like most other adaptive learning algo-
rithms, NetLines combines a growth heuristics with a particular learningalgorithm for training the individual units, which are simple perceptrons.In this section, we present the growth heuristics ﬁrst, followed by the de-scription of Minimerror, our perceptron learning algorithm.
We ﬁrst introduce the following useful remark: if a neuron has to learn a
target¿, and the learned state turns out to be ¾, then the product ¾¿D1i f
the target has been correctly learned, and ¾¿D¡1 otherwise.
Given a maximal accepted number of hidden units, H
max, and a maximal
number of tolerated training errors, Emax, the Netlines algorithm may be
summarized as follows:
Algorithm.
†Initialize
hD0;
setthe targets¿„
hC1D¿„for„D1;:::; P;1012 J. Manuel Torres Moreno and Mirta B. Gordon
†Repeat
1. /* train the hidden units */
hDhC1; /* connect hidden unit hto the inputs */
learn the training setfE»„;¿„
hg,„D1;:::; P;
after learning, ¾„
hDsign.Ewh¢E»„/,„D1;:::; P;
ifhD1 /* for the ﬁrst hidden neuron */
if¾„
1D¿„
18„then stop. /* the training set is LS */;
else set¿„
hC1D¾„
h¿„for„D1;:::; P;go to 1;
end if
2. /* learn the mapping between the IRs and the outputs */
connect the output neuron to the htrained hidden units;
learn the training setfE¾„.h/;¿„g;„D1;:::; P;
after learning, ‡„.h/Dsign‡
EW.h/¢E¾„·
,„D1;:::; P;
set¿„
hC1D‡„¿„for„D1;:::; P;
count the number of training errors eDP
„.1¡¿„
hC1/=2;
†Until.hDHmaxore•Emax/;
The generated network has HDhhidden units. In the appendix we present
a solution to the learning strategy with a bounded number of hidden units.In practice, the algorithm ends up with much smaller networks than thisupper bound, as will be shown in section 5.
2.4 The Perceptron Learning Algorithm. The ﬁnal number of hidden
neurons, which are simple perceptrons, depends on the performance of thelearning algorithm used to train them. The best solution should minimizethe number of errors. If the training set is LS, it should endow the units withthe lowest generalization error. Our incremental algorithm uses Minimerror(Gordon & Berchier, 1993) to train the hidden and output units. Minimer-ror is based on the minimization of a cost function Ethat depends on the
perceptron weights Ewthrough the stabilities of the training patterns. If the
input vector isE»
„and¿„the corresponding target, then the stability °„of
pattern„is a continuous and derivable function of the weights, given by:
°„D¿„Ew¢E»„
kEwk; (2.3)
wherekEwkDp
Ew¢Ew. The stability is independent of the norm of the weights
kEwk. It measures the distance of the pattern to the separating hyperplane,
which is normal to Ew; it is positive if the pattern is well classiﬁed, negativeClassiﬁcation Tasks with Binary Units 1013
otherwise. The cost function Eis:
ED1
2PX
„D1•
1¡tanh°„
2T‚
: (2.4)
The contribution to Eof patterns with large negative stabilities is ’1, that
is, they are counted as errors, whereas the contribution of patterns withlarge, positive stabilities is vanishingly small. Patterns at both sides of thehyperplane within a window of width …4Tcontribute to the cost function
even if they have positive stability.
The properties of the global minimum of equation 2.4 have been studied
theoretically with methods of statistical mechanics (Gordon & Grempel,1995). It was shown that in the limit T!0, the minimum of Ecorresponds
to the weights that minimize the number of training errors. If the trainingset is LS, these weights are not unique (Gyorgyi & Tishby, 1990). In that case,there is an optimal learning temperature such that the weights minimizingEat that temperature endow the perceptron with a generalization error
numerically indistinguishable from the optimal (Bayesian) value.
The algorithm Minimerror (Gordon & Berchier, 1993; Rafﬁn & Gordon,
1995) implements a minimization of Erestricted to a subspace of normalized
weights, through a gradient descent combined with a slow decrease of thetemperature T, which is equivalent to a deterministic annealing. It has been
shown that the convergence is faster if patterns with negative stabilities areconsidered at a temperature T
¡larger than those with positive stabilities,
TC, with a constant ratio µDT¡=TC. The weights and the temperatures are
iteratively updated through:
–Ew.t/D†"X
„=°„•0¿„E»„
cosh2.°„=2T¡/CX
„=°„>0¿„E»„
cosh2.°„=2TC/#
(2.5)
T¡1
C.tC1/DT¡1
C.t/C–T¡1IT¡DµTCI (2.6)
Ew.tC1/Dp
NC1Ew.t/C–Ew.t/
kEw.t/C–Ew.t/k: (2.7)
Notice from equation 2.5 that only the incorrectly learned patterns at dis-
tances shorter than …2T¡from the hyperplane, and those correctly learned
lying closer than…2TC, contribute effectively to learning. The contribu-
tion of patterns outside this region is vanishingly small. By decreasing thetemperature, the algorithm selects to learn patterns increasingly localizedin the neighborhood of the hyperplane, allowing for a highly precise de-termination of the parameters deﬁning the hyperplane, which are the neu-ron’s weights. Normalization 2.7 restricts the search to the subspace withkEwkDp
NC1.
The only adjustable parameters of the algorithm are the temperature ratio
µDT¡=TC, the learning rate †, and the annealing rate –T¡1. In principle,1014 J. Manuel Torres Moreno and Mirta B. Gordon
they should be adapted to each speciﬁc problem. However, as a result of
our normalizing the weights top
NC1 and to data standardization (see the
next section), all the problems are brought to the same scale, simplifying thechoice of the parameters.
2.5 Data Standardization. Instead of determining the best parameters
for each new problem, we standardize the input patterns of the training setthrough a linear transformation, applied to each component:
Q»
„
iD»„
i¡h»ii
1iI1•i•N: (2.8)
The meanh»iiand the variance42
i, deﬁned as usual,
h»iiD1
PPX
„D1»„
i (2.9)
1i2D1
PPX
„D1.»„
i¡h»ii/2D1
PPX
„D1.»„
i/2¡.h»ii/2; (2.10)
need only a single pass of the Ptraining patterns to be determined. After
learning, the inverse transformation is applied to the weights,
Qw0Dp
NC1w0¡NP
iD1wih»ii=1i
rh
w0¡PN
jD1wjh»ji=1ji2
CPN
jD1.wj=1j/2(2.11)
QwiDp
NC1wi=1irh
w0¡PN
jD1wjh»ji=1ji2
CPN
jD1.wj=1j/2; (2.12)
so that the normalization (see equation 2.8) is completely transparent to the
user: with the transformed weights (see equations 2.11 and 2.12), the neuralclassiﬁer is applied to the data in the original user’s units, which do notneed to be renormalized.
As a consequence of the weights scaling (see equation 2.7) and the in-
puts standardization (see equation 2.8), all the problems are automaticallyrescaled. This allows us to use always the same values of Minimerror’s pa-rameters: the standard values †D0:02,–T
¡1D10¡3, andµD6. They were
used throughout this article, the reported results being highly insensitive toslight variations of them. However, in some extremely difﬁcult cases, likelearning the parity in dimensions N>10 and ﬁnding the separation of the
sonar signals (see section 5), larger values of µwere needed.Classiﬁcation Tasks with Binary Units 1015
2.6 Interpretation. It has been shown (Gordon, Peretto, & Berchier, 1993)
that the contribution of each pattern to the cost function of Minimerror,[1¡tanh.°
„=2T/]=2, may be interpreted as the probability of misclassiﬁca-
tion at the temperature Tat which the minimum of the cost function has
been determined. By analogy, the neuron’s prediction on a new input E»may
be given a conﬁdence measure by replacing the (unknown) pattern stabil-ity by its absolute value k°kDkEw¢E»k=kEwk, which is its distance to the
hyperplane. This interpretation of the sigmoidal function tanh .k°k=2T/as
the conﬁdence on the neuron’s output is similar to the one proposed earlier(Goodman, Smyth, Higgins, & Miller, 1992) within an approach based oninformation theory.
The generalization of these ideas to multilayered networks is not straight-
forward. An estimate of the conﬁdence on the classiﬁcation by the outputneuron should include the magnitude of the weighted sums of the hiddenneurons, as they measure the distances of the input pattern to the domainboundaries. However, short distances to the separating hyperplanes are notalways correlated to low conﬁdence on the network’s output. For an exam-ple, we refer again to Figure 1. Consider a pattern lying close to hyperplane1. A small, weighted sum on neuron 1 may cast doubt on the classiﬁcationif the pattern’s IR is ( ¡CC ) but not if it is (¡C¡ ), because a change of the
sign of the weighted sum in the latter case will map the pattern to the IR(CC¡ ) which, being another IR of the same class, will be given the same
output by the network. It is worth noting that the same difﬁculty is met bythe interpretation of the outputs of multilayered perceptrons, trained withbackpropagation, as posterior probabilities. We do not explore this problemany further because it is beyond the scope of this article.
3 Comparison with Other Strategies
There are few learning algorithms for neural networks composed of binary
units. To our knowledge, all of them are incremental. In this section, wegive a short overview of some of them, in order to put forward the maindifferences with NetLines. We discuss the growth heuristics and then theindividual unit training algorithms.
The Tiling algorithm (M´ ezard & Nadal, 1989) introduces hidden layers,
one after the other. The ﬁrst neuron of each layer is trained to learn an IR thathelps to decrease the number of training errors; supplementary hidden unitsare then appended to the layer until the IRs of all the patterns in the train-ing set are faithful. This procedure may generate very large networks. TheUpstart algorithm (Frean, 1990) introduces successive couples of daughterhidden units between the input layer and the previously included hiddenunits, which become their parents. The daughters are trained to correctthe parents’ classiﬁcation errors, one daughter for each class. The obtainednetwork has a treelike architecture. There are two different algorithms im-plementing the Tilinglike Learning in the Parity Machine (Biehl & Opper,1016 J. Manuel Torres Moreno and Mirta B. Gordon
1991), Offset (Martinez & Est` eve, 1992), and MonoPlane (Torres Moreno &
Gordon, 1995). In both, each appended unit is trained to correct the errorsof the previously included unit in the same hidden layer, a procedure thathas been shown to generate a parity machine: the class of the input patternsis the parity of the learned IRs. Unlike Offset, which implements the paritythrough a second hidden layer that needs to be pruned, MonoPlane goeson adding hidden units (if necessary) in the same hidden layer until thenumber of training errors at the output vanishes. Convergence proofs forbinary input patterns have been produced for all these algorithms. In thecase of real-valued input patterns, a solution to the parity machine with abounded number of hidden units also exists (Gordon, 1996).
The rationale behind the construction of the parity machine is that it
is not worth training the output unit before all the training errors of thehidden units have been corrected. However, Marchand, Golea, and Ruj´ an
(1990) pointed out that it is not necessary to correct all the errors of thesuccessively trained hidden units. It is sufﬁcient that the IRs be faithful andLS. If the output unit is trained immediately after each appended hiddenunit, the network may discover that the IRs are already faithful and stopadding units. This may be seen in Figure 1. None of the parity machineimplementations would ﬁnd the solution represented on the ﬁgure, becauseeach of the three perceptrons systematically unlearns part of the patternslearned by the preceding one.
To our knowledge,
Sequential Learning (Marchand et al., 1990) is the
only incremental learning algorithm that might ﬁnd a solution equivalent(although not the same) to the one of Figure 1. In this algorithm, the ﬁrstunit is trained to separate the training set keeping one “pure” half-space—containing patterns of only one class. Wrongly classiﬁed patterns, if any,must all lie in the other half-space. Each appended neuron is trained toseparate wrongly classiﬁed patterns with this constraint of always keepingone pure, error-free half-space. Thus, neurons must be appended in a preciseorder, making the algorithm difﬁcult to implement in practice. For example,
Sequential Learning applied to the problem of Figure 1 needs to impose that
the ﬁrst unit ﬁnds the weights Ew3, the only solution satisfying the purity
restriction.
Other proposed incremental learning algorithms strive to solve the prob-
lem with different architectures, and/or with real valued units. For example,in the algorithm
Cascade Correlation (Fahlman & Lebiere, 1990), each ap-
pended unit is selected among a pool of several real-valued neurons, trainedto learn the correlation between the targets and the training errors. The unitis then connected to the input units and to all the other hidden neuronsalready included in the network.
Another approach to learning classiﬁcation tasks is through the construc-
tion of decision trees (Breiman, Friedman, Olshen, & Stone, 1984), which hi-erarchically partition the input space through successive dichotomies. Theneural networks implementations generate treelike architectures. Each neu-Classiﬁcation Tasks with Binary Units 1017
ron of the tree introduces a dichotomy of the input space, which is treated
separately by the children nodes, which eventually produce new splits. Be-sides the weights, the resulting networks need to store the decision path.The proposed heuristics (Sirat & Nadal, 1990; Farrell & Mammone, 1994;Knerr, Personnaz, & Dreyfus, 1990) differ in the algorithm used to train eachnode and/or in the stopping criterion. In particular, Neural-Trees (Sirat &Nadal, 1990) may be regarded as a generalization of Classiﬁcation and Re-gression Trees (CART) (Breiman et al., 1984) in which the hyperplanes arenot constrained to be perpendicular to the coordinate axis. The heuristics ofthe Modiﬁed Neural Tree Network (MNTN) (Farrell & Mammone, 1994),similar to Neural-Trees, includes a criterion of early stopping based on aconﬁdence measure of the partition. As NetLines considers the whole inputspace to train each hidden unit, it generates domain boundaries that maygreatly differ from the splits produced by trees. We are not aware of anysystematic study or theoretical comparison of both approaches.
Other algorithms, like Restricted Coulomb Energy (RCE) (Reilly, Cooper,
& Elbaum, 1982), Grow and Learn (GAL) (Alpaydin, 1990), Glocal (Depe-nau, 1995), and Growing Cells (Fritzke, 1994), propose to cover or mask theinput space with hyperspheres of adaptive size containing patterns of thesame class. These approaches generally end up with a very large number ofunits. Covering Regions by the LP Method (Mukhopadhyay, Roy, Kim, &Govil, 1993) is a trial-and-error procedure devised to select the most efﬁcientmasks among hyperplanes, hyperspheres, and hyperellipsoids. The mask’sparameters are determined through linear programming.
Many incremental strategies use the Pocket algorithm (Gallant, 1986)
to train the appended units. Its main drawback is that it has no naturalstopping condition, which is left to the user’s patience. The proposed alter-native algorithms (Frean, 1992; Bottou & Vapnik, 1992) are not guaranteedto ﬁnd the best solution to the problem of learning. The algorithm used bythe MNTN (Farrell & Mammone, 1994) and the ITRULE (Goodman et al.,1992) minimize cost functions similar to equation 2.4, but using differentmisclassiﬁcation measures at the place of our stability (see equation 2.3).The essential difference with Minimerror is that none of these algorithms isable to control which patterns contribute to learning, as Minimerror doeswith the temperature.
4 Generalization to Multiclass Problems
The usual way to cope with problems having more than two classes is to
generate as many networks as classes. Each network is trained to separatepatterns of one class from all the others, and a winner-takes-all (WTA) strat-egy based on the value of the output’s weighted sum in equation 2.2 is usedto decide the class if more than one network recognizes the input pattern. Inour case, because we use normalized weights, the output’s weighted sumis merely the distance of the IR to the separating hyperplane. All the pat-1018 J. Manuel Torres Moreno and Mirta B. Gordon
terns mapped to the same IR are given the same output’s weighted sum,
independent of the relative position of the pattern in input space. A strongweighted sum on the output neuron is not inconsistent with small weightedsums on the hidden neurons. Therefore, a naive WTA decision may not givegood results, as shown in the example in section 5.3.1.
We now describe an implementation for the multiclass problem that re-
sults in a treelike architecture of networks. It is more involved than the naiveWTA and may be applied to any binary classiﬁer. Suppose that we have aproblem with Cclasses. We must choose in which order the classes will
be learned, say .c
1;c2;:::; cC/. This order constitutes a particular learning
sequence. Given a particular learning sequence, a ﬁrst network is trainedto separate class c
1, which is given output target ¿1DC 1, from the others
(which are given targets ¿1D¡ 1). The opposite convention is equivalent
and could equally be used. After training, all the patterns of class c1are
eliminated from the training set, and we generate a second network trainedto separate patterns of class c
2from the remaining classes. The procedure,
reiterated with training sets of decreasing size, generates C¡1 hierarchi-
cally organized tree of networks (TON): the outputs are ordered sequences
E‡D.‡1;‡2;:::;‡ C¡1/. The predicted class of a pattern is ci, where iis the
ﬁrst network in the sequence having an output C1(‡iDC1 and‡jD¡1 for
j<i), the outputs of the networks with j>ibeing irrelevant.
The performance of the TON may depend on the chosen learning se-
quence. Therefore, it is convenient that an odd number of TONs, trainedwith different learning sequences, compete through a vote. We veriﬁed em-pirically, as is shown in section 5.3, that this vote improves the results ob-tained with each of the individual TONs participating in the vote. Noticethat our procedure is different from bagging (Breiman, 1994); all the net-works of the TON are trained with the same training set, without the need
of any resampling procedure.
5 Applications
Although convergence proofs of learning algorithms are satisfactory on the-
oretical grounds, they are not a guarantee of good generalization. In fact,they demonstrate only that correct learning is possible; they do not addressthe problem of generalization. This last issue still remains quite empirical(Vapnik, 1992; Geman et al., 1992; Friedman, 1996), and the generalizationperformance of learning algorithms is usually tested on well-known bench-marks (Prechelt, 1994).
We ﬁrst tested the algorithm on learning the parity function of Nbits for
2•N•11. It is well known that the smallest network with the architecture
considered here needs HDNhidden neurons. The optimal architecture
was found in all the cases. Although this is quite an unusual performance,the parity is not a representative problem: learning is exhaustive, and gen-eralization cannot be tested. Another test, the classiﬁcation of sonar signalsClassiﬁcation Tasks with Binary Units 1019
(Gorman & Sejnowski, 1988), revealed the quality of Minimerror, as it solved
the problem without hidden units. In fact, we found that not only the train-ing set of this benchmark is linearly separable, a result already reported(Hoehfeld & Fahlman, 1991; Roy, Kim, & Mukhopadhyay, 1993), but thatthe complete database—the training and the test sets together—is also lin-early separable (Torres Moreno & Gordon, 1998).
We next present our results, generalization error †
gand number of weights,
on several benchmarks corresponding to different kinds of problems: binaryclassiﬁcation of binary input patterns, binary classiﬁcation of real-valuedinput patterns, and multiclass problems. These benchmarks were chosenbecause they have already served as a test for many other algorithms, pro-viding us with unbiased results for comparison. The generalization error†
gof NetLines was estimated as usual, through the fraction of misclassiﬁed
patterns on a test set of data.
The results are reported as a function of the training sets sizes Pwhenever
these sizes are not speciﬁed by the benchmark. Besides the generalizationerror†
g, averaged over a (speciﬁed) number of classiﬁers trained with ran-
domly selected training sets, we also present the number of weights of thecorresponding networks which is a measure of the classiﬁer’s complexity,as it corresponds to the number of its parameters.
Training times are usually cited among the characteristics of the training
algorithms. Only the numbers of epochs used by backpropagation on twoof the studied benchmarks have been published; we restrict the comparisonto these cases. As NetLines updates only Nweights per epoch, whereas
backpropagation updates all the network’s weights, we compare the totalnumber of weights updates. They are of the same order of magnitude forboth algorithms. However, these comparisons should be taken with cau-tion. NetLines is a deterministic algorithm; it learns the architecture andthe weights through a single run, whereas with backpropagation severalarchitectures must be previously investigated, and this time is not includedin the training time.
The following notation is used: Dis the total number of available patterns,
Pthe number of training patterns, and Gthe number of test patterns.
5.1 Binary Inputs. The case of binary input patterns has the property,
not shared by real-valued inputs, that every pattern may be separated fromthe others by a single hyperplane. This solution, usually called grandmother ,
needs as many hidden units as patterns in the training set. In fact, the conver-gence proofs for incremental algorithms in the case of binary input patternsare based on this property.
5.1.1 Monk’s Problem. This benchmark, thoroughly studied with many
different learning algorithms (Trhun et al., 1991), contains three distinctproblems. Each has an underlying logical proposition that depends on sixdiscrete variables, coded with ND17 binary numbers. The total number of1020 J. Manuel Torres Moreno and Mirta B. Gordon
possible input patterns is DD432, and the targets correspond to the truth ta-
ble of the corresponding proposition. Both NetLines and MonoPlane foundthe underlying logical proposition of the ﬁrst two problems; they general-ized correctly, giving †
gD0. In fact, these are easy problems: all the neural
network–based algorithms, and some nonneural learning algorithms werereported to generalize them correctly. In the third Monk’s problem, 6 pat-terns among the P
3D122 examples are given wrong targets. The general-
ization error is calculated over the complete set of DD432 patterns, that is,
including the training patterns, but in the test set all the patterns are giventhe correct targets. Thus, any training method that learns the training setcorrectly will make at least 1 :4% of generalization errors. Four algorithms
specially adapted to noisy problems were reported to reach †
gD0. However,
none of them generalizes correctly the two other (noiseless) Monk’s prob-lems. Besides them, the best performance, †
gD0:0277, which corresponds
to 12 misclassiﬁed patterns, is reached only by neural networks methods:backpropagation, backpropagation with weight decay, cascade correlation,and NetLines. The number of hidden units generated with NetLines (58weights) is intermediate between backpropagation with weight decay (39)and cascade correlation (75) or backpropagation (77). MonoPlane reached aslightly worse performance ( †
gD0:0416, or 18 misclassiﬁed patterns) with
the same number of weights as NetLines, showing that the parity machineencoding may not be optimal.
5.1.2 Two or More Clumps. In this problem (Denker et al., 1987) the net-
work has to discriminate if the number of clumps in a ring of Nbits is strictly
smaller than 2 or not. One clump is a sequence of identical bits bounded bybits of the other kind. The patterns are generated through a Monte Carlomethod in which the mean number of clumps is controlled by a parameterk(M´ezard & Nadal, 1989). We generated training sets of Ppatterns with
kD3, corresponding to a mean number of clumps of …1:5, for rings of
ND10 and ND25 bits. The generalization error corresponding to sev-
eral learning algorithms, estimated with independently generated testingsets of the same sizes as the training sets, GDP, are displayed in Figure 2
as a function of P. Points with error bars correspond to averages over 25
independent training sets. Points without error bars correspond to best re-sults. NetLines, MonoPlane, and Upstart for ND25 have nearly the same
performances when trained to reach error-free learning.
We tested the effect of early stopping by imposing on NetLines a maximal
number of two hidden units ( HD2). The residual training error †
tis plotted
on Figure 2, as a function of P. Note that early stopping does not help to de-
crease†g. Overﬁtting, which arises when NetLines is applied until error-free
training is reached, does not degrade the network’s generalization perfor-mance. This behavior is very different from the one of networks trainedwith backpropagation. The latter reduces classiﬁcation learning to a regres-sion problem, in which the generalization error can be decomposed in twoClassiﬁcation Tasks with Binary Units 1021
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13 /G19/G13/G13/G31/G48/G57/G2F/G4C/G51/G48/G56 ε/G57/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G2B/G20/G15/G0C
/G31/G48/G57/G2F/G4C/G51/G48/G56/G37/G4C/G4F/G4C/G51/G4A/G0F/G2A/G55/G52/G5A/G57/G4B
/G38/G53/G56/G57/G44/G55/G57
/G33/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G15/G18
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13/G13/G11/G13/G13/G11/G14/G13/G11/G15/G13/G11/G16/G13/G11/G17/G13/G11/G18
ε/G4A
/G33/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G14/G13/G25/G44/G46/G4E/G53/G55/G52/G53
/G36/G57/G48/G53/G5A/G4C/G56/G48
/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56
Figure 2: Two or more clumps for two ring sizes, ND10 and ND25. Gen-
eralization error †gversus size of the training set P, for different algorithms.
ND10: backpropagation (Solla, 1989), Stepwise (Knerr et al., 1990). ND25:
Tiling (M´ ezard & Nadal, 1989), Upstart (Frean, 1990). Results with the Growth
Algorithm (Nadal, 1989) are indistinguishable from those of Tiling at the scaleof the ﬁgure. Points without error bars correspond to best results. Results ofMonoPlane and NetLines are averages over 25 tests.
competing terms: bias and variance. With backpropagation, early stopping
helps to decrease overﬁtting because some hidden neurons do not reachlarge enough weights to work in the nonlinear part of the sigmoidal trans-fer functions. All the neurons working in the linear part may be replaced bya single linear unit. Thus, with early stopping, the network is equivalent toa smaller one with all the units working in the nonlinear regime. Our resultsare consistent with recent theories (Friedman, 1996) showing that, contraryto regression, the bias and variance components of the generalization errorin classiﬁcation combine in a highly nonlinear way.
The number of weights used by the different algorithms is plotted on a
logarithmic scale as a function of Pin Figure 3. It turns out that the strategy
of NetLines is slightly better than that of MonoPlane with respect to bothgeneralization performance and network size.
5.2 Real Valued Inputs. We tested NetLines on two problems that have
real valued inputs (we include graded-valued inputs here).
5.2.1 Wisconsin Breast Cancer Database. The input patterns of this bench-
mark (Wolberg & Mangasarian, 1990) have ND9 attributes characterizing1022 J. Manuel Torres Moreno and Mirta B. Gordon
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13 /G19/G13/G13/G31/G48/G57/G2F/G4C/G51/G48/G56
/G38/G53/G56/G57/G44/G55/G57/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G15/G18
/G33/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13/G14/G13/G14/G13/G13/G14/G13/G13/G13
/G33/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G14/G13
/G25/G44/G46/G4E/G53/G55/G52/G53/G44/G4A/G44/G57/G4C/G52/G51/G36/G57/G48/G53/G5A/G4C/G56/G48
/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56
Figure 3: Two or more clumps. Number of weights (logarithmic scale) versus
size of the training set P, for ND10 and ND25. Results of MonoPlane and
NetLines are averages over 25 tests. The references are the same as in Figure 2.
samples of breast cytology, classiﬁed as benign or malignant. We excluded
from the original database 16 patterns that have the attribute »6(“bare nu-
clei”) missing. Among the remaining DD683 patterns, the two classes are
unevenly represented, 65.5% of the examples being benign. We studied thegeneralization performance of networks trained with sets of several sizes P.
The Ppatterns for each learning test were selected at random. In Figure 4a,
the generalization error at classifying the remaining G·D¡Ppatterns is
displayed as a function of the corresponding number of weights in a loga-rithmic scale. For comparison, we included in the same ﬁgure results of asingle perceptron trained with PD75 patterns using Minimerror. The re-
sults, averaged values over 50 independent tests for each P, show that both
NetLines and MonoPlane have lower †
gand fewer parameters than other
algorithms on this benchmark.
The total number of weights updates needed by NetLines, including the
weights of the dropped output units, is 7 ¢104; backpropagation needed
…104(Prechelt, 1994).
The trained network may be used to classify the patterns with missing
attributes. The number of misclassiﬁed patterns among the 16 cases forwhich attribute »
6is missing is plotted as a function of the possible values
of»6on Figure 4b. For large values of »6, there are discrepancies between the
medical and the network’s diagnosis on half the cases. This is an exampleof the kind of information that may be obtained in practical applications.Classiﬁcation Tasks with Binary Units 1023
/G14/G13 /G14/G13/G13/G13/G11/G13/G13/G13/G11/G13/G14/G13/G11/G13/G15/G13/G11/G13/G16/G13/G11/G13/G17/G13/G11/G13/G18/G13/G11/G13/G19
/G14/G15
/G17/G16
/G18
/G1A
/G19
/G30/G52/G51/G52/G33/G4F/G44/G51/G48 /G0B/G33/G20/G14/G19/G13/G0C/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G33/G20/G14/G19/G13/G0C/G30/G4C/G51/G4C/G50/G48/G55/G55/G52/G55 /G0B/G33/G20/G1A/G18/G0C/G25/G55/G48/G44/G56/G57 /G46/G44/G51/G46/G48/G55 /G0B/G44/G0C
/G30/G52/G51/G52/G33/G4F/G44/G51/G48/G31/G48/G57/G2F/G4C/G51/G48/G56ε/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56/G14 /G15/G16/G17 /G18/G19 /G1A/G1B/G1C /G14 /G13
/G33/G52/G56/G56/G4C/G45/G4F/G48 /G59/G44/G4F/G58/G48/G56 /G52/G49 /G44/G57/G57/G55/G4C/G45/G58/G57/G48 ξ/G19/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56/G25/G55/G48/G44/G56/G57 /G46/G44/G51/G46/G48/G55 /G0B/G45/G0C
Figure 4: Breast cancer classiﬁcation. (a) Generalization error †gversus num-
ber of weights (logarithmic scale), for PD525. 1–3: Rprop with no shortcuts
(Prechelt, 1994); 4–6: Rprop with shortcuts (Prechelt, 1994); 7: Cascade Correla-tion (Depenau, 1995). For comparison, results with smaller training sets, PD75
(single perceptron) and PD160, are displayed. Results of MonoPlane and Net-
Lines are averages over 50 tests. (b) Classiﬁcation errors versus possible valuesof the missing attribute bare nuclei for the 16 incomplete patterns, averagedover 50 independently trained networks.
5.2.2 Diabetes Diagnosis. This benchmark (Prechelt, 1994) contains DD
768 patterns described by ND8 real-valued attributes, corresponding to
…35% of Pima women suffering from diabetes, 65% being healthy. Training
sets of PD576 patterns were selected at random, and generalization was
tested on the remaining GD192 patterns. The comparison with published
results obtained with other algorithms tested under the same conditions,presented in Figure 5, shows that NetLines reaches the best performancepublished so far on this benchmark, needing many fewer parameters. Train-ing times of NetLines are of …10
5updates. The numbers of updates needed
by Rprop (Prechelt, 1994) range between 4 ¢103and 5¢105, depending on
the network’s architecture.
5.3 Multiclass Problems. We applied our learning algorithm to two dif-
ferent problems, both of three classes. We compare the results obtained witha WTA classiﬁcation based on the results of three networks, each indepen-dently trained to separate one class from the two others, to the results ofthe TON architectures described in section 4. Because the number of classesis low, we determined the three TONs, corresponding to the three possible1024 J. Manuel Torres Moreno and Mirta B. Gordon
/G14/G13 /G14/G13/G13/G13/G11/G15/G13/G13/G11/G15/G15/G13/G11/G15/G17/G13/G11/G15/G19/G13/G11/G15/G1B/G13/G11/G16/G13
/G14
/G15/G16
/G18/G19
/G17
/G31/G48/G57/G2F/G4C/G51/G48/G56/G2C/G51/G47/G4C/G44/G51/G56 /G33/G4C/G50/G44 /G27/G4C/G44/G45/G48/G57/G48/G56
ε/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56
Figure 5: Diabetes diagnosis: Generalization error †gversus number of weights.
Results of NetLines are averages over 50 tests. 1–3: Rprop no shortcuts, 4–6:Rprop with shortcuts (Prechelt, 1994).
learning sequences. The vote of the three TONs improves the performances,
as expected.
5.3.1 Breiman’s Waveform Recognition Problem. This problem was intro-
duced as a test for the algorithm CART (Breiman et al., 1984). The inputpatterns are deﬁned by ND21 real-valued amplitudes x.t/observed at reg-
ularly spaced intervals tD1;2;:::; N. Each pattern is a noisy convex linear
combination of two among three elementary waves (triangular waves cen-tered on three different values of t). There are three possible combinations,
and the pattern’s class identiﬁes from which combination it is issued.
We trained the networks with the same 11 training sets of PD300 ex-
amples, and generalization was tested on the same independent test setofGD5000, as in Gascuel (1995). Our results are displayed in Figure 6,
where only results of algorithms reaching †
g<0:25 in Gascuel (1995) are
included. Although it is known that due to the noise, the classiﬁcation errorhas a lower bound of …14% (Breiman et al., 1984), the results of NetLines
and MonoPlane presented here correspond to error-free training. The net-works generated by NetLines have between three and six hidden neurons,depending on the training sets. The results obtained with a single percep-tron trained with Minimerror and with the perceptron learning algorithm,which may be considered the extreme case of early stopping, are hardly im-proved by the more complex networks. Here again the overﬁtting producedby error-free learning with NetLines does not cause the generalization per-Classiﬁcation Tasks with Binary Units 1025
/G14/G13 /G14/G13/G13 /G14/G13/G13/G13 /G14/G13/G13/G13/G13 /G14/G13/G13/G13/G13/G13/G13/G11/G14/G17/G13/G11/G14/G19/G13/G11/G14/G1B/G13/G11/G15/G13/G13/G11/G15/G15/G13/G11/G15/G17/G13/G11/G15/G19
/G14
/G15 /G16/G17
/G18/G19
/G1A/G1B/G30/G52/G51/G52/G33/G4F/G44/G51/G48 /G0B/G3A/G37/G24/G0C
/G37/G4B/G48/G52/G55/G48/G57/G4C/G46/G44/G4F /G4F/G4C/G50/G4C/G57/G30/G4C/G51/G4C/G50/G48/G55/G55/G52/G55
/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G39/G52/G57/G48/G0C/G25/G55/G48/G4C/G50/G44/G51/G0A/G56 /G3A/G44/G59/G48/G49/G52/G55/G50/G56
ε/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G53/G44/G55/G44/G50/G48/G57/G48/G55/G56
Figure 6: Breiman waveforms: Generalization error †gaveraged over 11 tests
versus number of parameters. Error bars on the number of weights generatedby NetLines and MonoPlane are not visible at the scale of the ﬁgure. 1: linear dis-crimination; 2: perceptron; 3: backpropagation; 4: genetic algorithm; 5: quadraticdiscrimination; 6: Parzen’s kernel; 7: K-NN; 8: constraint (Gascuel, 1995).
formance to deteriorate. The TONs vote reduces the variance but does not
decrease the average †g.
5.3.2 Fisher’s Iris Plants Database. In this classic three-class problem, one
has to determine the class of iris plants based on the values of ND4 real-
valued attributes. The database of DD150 patterns contains 50 examples
of each class. Networks were trained with PD149 patterns, and the gener-
alization error is the mean value of all the 150 leave-one-out possible tests.Results of†
gare displayed as a function of the number of weights in Figure 7.
Error bars are available for only our own results. In this difﬁcult problem,the vote of the three possible TONs trained with the three possible classsequences (see section 4) improves the generalization performance.
6 Conclusion
We presented an incremental learning algorithm for classiﬁcation, which we
call NetLines. It generates small feedforward neural networks with a singlehidden layer of binary units connected to a binary output neuron. NetLinesallows for an automatic adaptation of the neural network to the complexityof the particular task. This is achieved by coupling an error-correcting strat-egy for the successive addition of hidden neurons with Minimerror, a very1026 J. Manuel Torres Moreno and Mirta B. Gordon
/G14/G13 /G14/G13/G13/G13/G11/G13/G13/G13/G11/G13/G15/G13/G11/G13/G17/G13/G11/G13/G19/G13/G11/G13/G1B/G13/G11/G14/G13
/G14/G15/G16/G0F/G17/G18
/G19
/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G59/G52/G57/G48/G0C/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G3A/G37/G24/G0C/G2C/G35/G2C/G36 /G47/G44/G57/G44/G45/G44/G56/G48
ε/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56
Figure 7: Iris database: Generalization error †gversus number of parameters.
1: offset, 2: backpropagation (Martinez & Est` eve, 1992); 4,5: backpropagation
(Verma & Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma& Mulawka, 1995).
efﬁcient perceptron training algorithm. Learning is fast not only because
it reduces the problem to that of training single perceptrons, but mainlybecause there is no longer a need for the usual preliminary tests required todetermine the correct architecture for the particular application. Theoremsvalid for binary as well as for real-valued inputs guarantee the existence ofa solution with a bounded number of hidden neurons obeying the growthstrategy.
The networks are composed of binary hidden units whose states consti-
tute a faithful encoding of the input patterns. They implement a mappingfrom the input space to a discrete H-dimensional hidden space, Hbeing
the number of hidden neurons. Thus, each pattern is labeled with a binaryword of Hbits. This encoding may be seen as a compression of the pattern’s
information. The hidden neurons deﬁne linear boundaries, or portions ofboundaries, between classes in input space. The network’s output may begiven a probabilistic interpretation based on the distance of the patterns tothese boundaries.
Tests on several benchmarks showed that the networks generated by our
incremental strategy are small, in spite of the fact that the hidden neuronsare appended until error-free learning is reached. Even when the networksobtained with NetLines are larger than those used by other algorithms, itsgeneralization error remains among the smallest values reported. In noisyor difﬁcult problems, it may be useful to stop the network’s growth beforeClassiﬁcation Tasks with Binary Units 1027
the condition of zero training errors is reached. This decreases overﬁtting, as
smaller networks (with less parameters) are thus generated. However, theprediction quality (measured by the generalization error) of the classiﬁersgenerated with NetLines is not improved by early stopping.
Our results were obtained without cross-validation or any data manip-
ulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,& Simard, 1993). Those costly procedures combine results of very largenumbers of classiﬁers, with the aim of improving the generalization perfor-mance through the reduction of the variance. Because NetLines is a stableclassiﬁer, presenting small variance, we do not expect that such techniqueswould signiﬁcantly improve our results.
Appendix
In this appendix we exhibit a particular solution to the learning strategy of
NetLines. This solution is built in such a way that the cardinal of a convexsubset of well-learned patterns, L
h, grows monotonically upon the addition
of hidden units. Because this cardinal cannot be larger than the total numberof training patterns, the algorithm must stop with a ﬁnite number of hiddenunits.
Suppose that hhidden units have already been included and that the
output neuron still makes classiﬁcation errors on patterns of the training set,called training errors. Among these wrongly learned patterns, let ”be the
one closest to the hyperplane normal to Ew
h, called hyperplane- hhereafter.
We deﬁne Lhas the subset of (correctly learned) patterns lying closer to
hyperplane- hthanE»”. Patterns in Lhhave 0<° h<j°”
hj. The subset Lhand
at least pattern ”are well learned if the next hidden unit, hC1, has weights:
EwhC1D¿”
hEwh¡.1¡†h/¿”
h.Ewh¢E»”/Oe0; (A.1)
whereOe0·.1;0;:::; 0/. The conditions that both Lhand pattern ”have
positive stabilities (are correctly learned) impose that
0<† h<min
„2Lhj°”
hj¡°„
h
j°”
hj: (A.2)
The following weights between the hidden units and the output will give
the correct output to pattern ”and to the patterns of Lh:
W0.hC1/DW0.h/C¿”(A.3)
Wi.hC1/DWi.h/for 1•i•h (A.4)
WhC1.hC1/D¡¿”: (A.5)
Thus, card.LhC1/‚card.Lh/C1. As the number of patterns in Lhincreases
monotonically with h, convergence is guaranteed with less than Phidden
units.1028 J. Manuel Torres Moreno and Mirta B. Gordon
Acknowledgments
J.M. thanks Consejo Nacional de Ciencia y Tecnolog´ ıa and Universidad
Aut´onoma Metropolitana, M´ exico, for ﬁnancial support (grant 65659).
References
Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un-
published doctoral dissertation, Ecole Polytechnique F´ ed´erale de Lausanne,
Switzerland.
Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical
Review A, 44 , 6888.
Bottou, L., & Vapnik, V . (1992). Local learning algorithms. Neural Computation,
4(6), 888–900.
Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department
of Statistics, University of California at Berkeley.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classiﬁcation
and regression trees . Monterey, CA: Wadsworth and Brooks/Cole.
Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopﬁeld, J.
(1987). Large automatic learning, rule extraction, and generalization. Complex
Systems, 1 , 877–922.
Depenau, J. (1995). Automated design of neural network architecture for classiﬁcation.
Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.
Drucker, H., Schapire, R., & Simard, P . (1993). Improving performance in neu-
ral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42–
49). San Mateo, CA: Morgan Kaufmann.
Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architec-
ture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,
2(pp. 524–532). San Mateo: Morgan Kaufmann.
Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree
networks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural
Information Processing Systems, 6 (pp. 1035–1042). San Mateo, CA: Morgan
Kaufmann.
Frean, M. (1990). The Upstart algorithm: A method for constructing and training
feedforward neural networks. Neural Computation, 2 (2), 198–209.
Frean, M. (1992). A “thermal” perceptron learning rule. Neural Computation, 4 (6),
946–957.
Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality
(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.
Fritzke, B. (1994). Supervised learning with growing cell structures. In
J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural informa-
tion processing systems, 6 (pp. 255–262). San Mateo, CA: Morgan Kaufmann.
Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern
Recognition, Oct. 28–31, Paris , vol. 4.Classiﬁcation Tasks with Binary Units 1029
Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).
5`emes Journ´ ees Nationales du PRC-IA Teknea, Nancy.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the
bias/variance dilemma. Neural Computation, 4 (1), 1–58.
Goodman, R. M., Smyth, P ., Higgins, C. M., & Miller, J. W. (1992). Rule-based
neural networks for classiﬁcation and probability estimation. Neural Compu-
tation, 4 (6), 781–804.
Gordon, M. B. (1996). A convergence theorem for incremental learning with real-
valued inputs. In IEEE International Conference on Neural Networks , pp. 381–
386.
Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule
that ﬁnds the optimal weights. In M. Verleysen (Ed.), European Symposium on
Artiﬁcial Neural Networks (pp. 105–110). Brussels: D Facto.
Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature
dependent algorithm. Europhysics Letters, 29 (3), 257–262.
Gordon, M. B., Peretto, P ., & Berchier, D. (1993). Learning algorithms for percep-
trons from statistical physics. Journal of Physics I (France), 3 , 377–387.
Gorman, R. P ., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered
network trained to classify sonar targets. Neural Networks, 1 , 75–89.
Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In
W. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses . Sin-
gapore: World Scientiﬁc.
Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using
the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:
Carnegie Mellon University.
Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A
stepwise procedure for building and training a neural network. In J. H´ erault
& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications
(pp. 41–50). Berlin: Springer-Verlag.
Marchand, M., Golea, M., & Ruj´ an, P . (1990). A convergence theorem for sequen-
tial learning in two-layer perceptrons. Europhysics Letters, 11 , 487–492.
Martinez, D., & Est` eve, D. (1992). The offset algorithm: Building and learning
method for multilayer neural networks. Europhysics Letters, 18 , 95–100.
M´ezard, M., & Nadal, J.-P . (1989). Learning in feedforward layered networks:
The Tiling algorithm. J. Phys. A: Math. and Gen., 22 , 2191–2203.
Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time al-
gorithm for generating neural networks for pattern classiﬁcation: Its stabilityproperties and some test results. Neural Computation, 5 (2), 317–330.
Nadal, J.-P . (1989). Study of a growth algorithm for a feedforward neural net-
work. Int. J. Neur. Syst., 1 , 55–59.
Prechelt, L. (1994). PROBEN1—A set of benchmarks and benchmarking rules for neu-
ral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,
Faculty of Informatics.
Rafﬁn, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror,
a temperature dependent learning algorithm. Neural Computation, 7 (6), 1206–
1224.1030 J. Manuel Torres Moreno and Mirta B. Gordon
Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category
learning. Biological Cybernetics, 45 , 35–41.
Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm
for the construction and training of a class of multilayer perceptron. Neural
Networks, 6 (1), 535–545.
Sirat, J. A., & Nadal, J.-P . (1990). Neural trees: A new tool for classiﬁcation.
Network, 1 , 423–438.
Solla, S. A. (1989). Learning and generalization in layered neural networks: The
contiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks
from Models to Applications . Paris: I.D.S.E.T.
Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled
with optimal perceptron learning for classiﬁcation. In M. Verleysen (Ed.),European Symposium on Artiﬁcial Neural Networks . Brussels: D Facto.
Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar
signals benchmark. Neural Proc. Letters, 7 (1), 1–4.
Trhun, S. B., et al. (1991). The monk’s problems: A performance comparison of different
learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie
Mellon University.
Vapnik, V . (1992). Principles of risk minimization for learning theory. In
J. E. Moody, S. J. Hanson, & R. P . Lippmann (Eds.), Advances in neural informa-
tion processing systems, 4 (pp. 831–838). San Mateo, CA: Morgan Kaufmann.
Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neu-
ral networks. In M. Verleysen (Ed.), European Symposium on Artiﬁcial Neural
Networks (pp. 359–364). Brussels: D Facto.
Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern
separation for medical diagnosis applied to breast cytology. In Proceedings of
the National Academy of Sciences, USA, 87 , 9193–9196.
Received February 13, 1997; accepted September 4, 1997.This article has been cited by:
1.C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral
analysis. IEEE Transactions on Neural Networks  10, 725-740. [ CrossRef ]
2.Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural
Networks. International Journal of Neural Systems  08, 647-659. [ CrossRef ]