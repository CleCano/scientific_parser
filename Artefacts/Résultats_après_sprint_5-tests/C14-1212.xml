<article>
	<preamble>C14-1212.pdf</preamble>
	<titre>Learning to Distinguish Hypernyms and Co-Hyponyms</titre>
	<auteurs>
		<auteur>
			<nom>Julie Weeds</nom>
			<mail>juliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Daoud Clarke</nom>
			<mail>N/A</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Jeremy Ref</nom>
			<mail>N/A</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>David Weir</nom>
			<mail>N/A</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Bill Keller</nom>
			<mail>N/A</mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract>  This work is concerned with distinguishing different semantic relations which exist between distributionally similar words. We compare a novel approach based on training a linear Support Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional similarity. We show that the new supervised approach does better even when there is minimal information about the target words in the training data, giving a 15% reduction in error rate over unsupervised approaches. </abstract>
	<introduction> Over recent years there has been much interest in the ﬁeld of distributional semantics, drawing on thedistributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds andWeir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of “nearest neighbours”)automatically and applied them in a variety of applications, generally with a good deal of success.In early research there was much interest in how these automatically generated thesauri compare withhuman-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classiﬁcation (Bollegalaet al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).A primary focus of distributional semantics has been on identifying words which are similar to eachother. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mixof synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically relatedwords. A central problem here is that whilst most measures of distributional similarity are symmetric,some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), anddetermines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the cohyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, issymmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of thewords cat,animal anddog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms1.1We read cat in the sense domestic cat rather than big cat , hence tiger is a co-hyponym rather than hyponymofcat.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedingsfooter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/2249cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23animal bird 0.36, ﬁsh 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, ﬁsh0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23Table 1: Top 15 neighbours of cat,animal anddog generated using Lin’s similarity measure (Lin,1998) considering all words and dependency features occurring 100 or more times in Wikipedia.Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it canbe useful to be able to distinguish between these different relationships. Consider the following twosentences.The cat ran across the road. (1)The animal ran across the road. (2)Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The abilityto determine whether entailment holds between the sentences, and in which direction, depends on theability to identify hyponymy. Given a similarity score of 0.29 between cat andanimal , how do weknow which is the hyponym and which is the hypernym?In applying distributional semantics to the problem of textual entailment, there is a need to generaliselexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relationsis crucial if approaches to the composition of distributional representations of meaning that are currentlyreceiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textualentailment problem.We formulate the challenge as follows: Consider a set of pairs of similar words /angbracketleftA, B/angbracketrightwhere one ofthree relationships hold between AandB:Alexically entails B,Blexically entails AorAandBarerelated by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section2, we discuss existing attempts to address this problem through the use of various directional measuresof distributional similarity.This paper considers the effectiveness of various supervised approaches, and makes the followingcontributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,achieving a signiﬁcant reduction in error rate in comparison to existing state-of-the-art methods basedon the notion of distributional generality. Second, by comparing two different data sets, one built fromBLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive importantinsights into the requirements of a valid evaluation of supervised approaches, and provide a data setfor further research in this area. Third, we show that when learning how to determine an ontologicalrelationship between a pair of similar words by means of the word’s distributional vectors, quite differentvector operations are useful when identifying different ontological relationships. In particular, using thedifference between the vectors for pairs of words is appropriate for the entailment task, whereas addingthe vectors works well for the co-hyponym task. </introduction>
	<discussion>  </discussion>
	<conclusion> Conclusions </conclusion>
	<biblio>  </biblio>
</article>
