<article>
	<preamble>b0e5c43edf116ce2909ae009cc27a1546f09.pdf</preamble>
	<titre>Inclusive yet Selective: Supervised Distributional Hypernymy Detection</titre>
	<auteurs>
		<auteur>
			<nom>Stephen Roller</nom>
			<mail>roller@cs.utexas.edu</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Katrin Erk</nom>
			<mail>katrin.erk@mail.utexas.edu</mail>
			<affiliation></affiliation>
		</auteur>
		<auteur>
			<nom>Gemma Boleda</nom>
			<mail>gemma.boleda@upf.edu</mail>
			<affiliation></affiliation>
		</auteur>
	</auteurs>
	<abstract>  We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. We ﬁnd that this hypothesis only holds when it is applied to relevant dimensions. We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion. </abstract>
	<introduction> One of the main criticisms of distributional models has been that they fail to distinguish between semanticrelations: Typical nearest neighbors of dogare words like cat,animal ,puppy ,tail, orowner , all obviouslyrelated to dog, but through very different types of semantic relations. On these grounds, Murphy (2002)argues that distributional models cannot be a valid model of conceptual representation. Distinguishingsemantic relations are also crucial for drawing inferences from distributional data, as different semanticrelations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such asRecognizing Textual Entailment or RTE (Geffet and Dagan, 2004).For these reasons, research has in recent years started to attempt the detection of speciﬁc semanticrelationships, and current results suggest that distributional models can, in fact, distinguish betweensemantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenciand Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTEand other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relationbetween a superordinate term in a taxonomy (e.g. animal ) and a subordinate term (e.g. dog).Distributional approaches to date for detecting hypernymy, and the related but broader relation oflexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been basedon the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffetand Dagan, 2009), which states that more speciﬁc terms appear in a subset of the distributional contextsin which more general terms appear. So, animal can occur in all the contexts in which dogcan occur,plus some contexts in which dogcannot – for instance, rights can be a typical cooccurrence for animal(e.g. “animal rights”), but not so much for dog(e.g. #“dog rights”).This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. Weshow that the current best unsupervised approach is brittle in that their performance depends on the spacethey are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,and if so, under what circumstances it holds. We use a simple supervised approach to relation detectionthat has good performance (accuracy .84 on B LESS , .85 on the lexical entailment dataset of Baroni etal. (2012)) and works well across different spaces.1Furthermore, we show that it can be interpretedas selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer isto propose the Selective Distributional Inclusion Hypothesis : The Distributional Inclusion Hypothesisholds, but only for relevant dimensions.This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footerare added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/1Code and data are available at http://stephenroller.com/research/coling14 .10252 BackgroundDistributional models. Distributional models represent a word through the contexts in which it hasbeen observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target wordis represented as a vector in a high-dimensional space in which the dimensions are context items (forexample, other words) and the coordinates of the vector indicate the target’s degree of association witheach context item. In this paper, we also use dimensionality reduced spaces in which dimensions do notstand for individual context items anymore.Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP 1]and other [NP 2]” implies that NP 2is a hypernym of NP 1. Pattern-based approaches have been appliedto meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al.,2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a widevariety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations.A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al.,2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors.Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004)introduce the notion of distributional generality , wherevis distributionally more general than uifuappears in a subset of the contexts in which vis found, and speculate that hypernyms ( v) should be moredistributionally general than hyponyms ( u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce theterm Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymyor the more loosely deﬁned relation of lexical entailment .Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here andin all equations below, uis the narrower term, and vthe more general one. Abusing notation, we write ufor both a word and its associated vector /angbracketleftu1,...,u n/angbracketright. Kotlerman et al. (2010) predict lexical entailmentwith the balAPinc measure, a modiﬁcation of the Average Precision (AP) measure (eq. 2). The generalnotion is that scores should increase with the number of dimensions of vthatushares, and also give moreweight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is capturedinAPinc by computing precision P(r)at every rank ramongu’s dimensions – where precision is thefraction of dimensions shared with v–, and weighting by the rank of the same dimension in the broaderterm,rel/prime(v,r,u ). The ﬁnal measure, balAPinc , smooths using the LINsimilarity measure (Lin, 1998).(We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).)1(x) =/braceleftBigg1ifx> 0;0otherwiseWeedsPrec (u,v) =/summationtextni=1ui·1(vi)/summationtextni=1ui(1)APinc (u,v) =/summationtext|1(u)|r=1P(r)·rel/prime(v,r,u ))|1(u)|(2)balAPinc (u,v) =/radicalbigAPinc (u,v)·LIN(u,v)TheClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the narrower termuhas lower values than vacross all dimensions (eq. 3). Lenci and Benotto (2012) introducetheinvCL measure, which uses ClarkeDE to measure both distributional inclusion of uinvand distributional non-inclusion ofvinu(eq. 4). While all other measures interpret the Distributional InclusionHypothesis as the degree to which a ⊆relation holds, Lenci and Benotto test the degree to which properinclusion /subsetnoteqlholds. They consider not only the degree to which the contexts of the narrower terms areincluded in the contexts of the wider term, but also determine the degree to which the wider term hascontexts that the narrower term does not have.1026CL(u,v) =/summationtextni=1min(ui,vi)/summationtextni=1ui(3)invCL (u,v) =/radicalbigCL(u,v)·(1−CL(v,u)) (4)Like Lenci and Benotto, we focus on the stricter hypernymy relation, rather than lexical entailment.We believe that the different relations that make up lexical entailment have different distributional indications and that, for that reason, it will be easier to detect the relations separately than together.Baroni et al. (2012) proposes a supervised approach to hypernymy detection that represents two wordsas the concatenation of their vectors. They also mention in passing another supervised approach thatrepresents two words as the component-wise difference of their vectors. These are broadly the twoapproaches that we test, though we introduce signiﬁcant modiﬁcations. </introduction>
	<discussion>  </discussion>
	<conclusion> Conclusion </conclusion>
	<biblio>  </biblio>
</article>
